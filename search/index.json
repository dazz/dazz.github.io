{"pages":[{"date":"2026-01-19","image":"https://blog.dazzlog.de/posts/2026-01-19_scripts-turn-into-system//hero.png","imageAlt":"","link":"https://blog.dazzlog.de/posts/2026-01-19_scripts-turn-into-system/","summary":"\u003cp\u003eA Makefile orchestrated a couple of Python scripts. For a few days, my setup felt almost boring — in a good way. I copied a YouTube video ID, pasted it into a Makefile, ran a command, and with a prompt to analyze the transcript, an AI produced Markdown and wrote it to a file. I copied the file to a GitHub repository, pushed it, and a static site generator turned it into something I actually enjoyed reading: HTML in the browser, and an RSS feed I could follow like a personal newspaper.\u003c/p\u003e","tags":["ai","ai engineering","guardrails","pydantic-ai","software development","spec driven development","symfony-ai","vibe-code","workflow"],"text":"a makefile orchestrated a couple of python scripts. for a few days, my setup felt almost boring — in a good way. i copied a youtube video id, pasted it into a makefile, ran a command, and with a prompt to analyze the transcript, an ai produced markdown and wrote it to a file. i copied the file to a github repository, pushed it, and a static site generator turned it into something i actually enjoyed reading: html in the browser, and an rss feed i could follow like a personal newspaper.\neverything lived as files. no database. no ui. no persistence beyond whatever ended up on disk.\nand for a while, that was perfect.\nthis approach allowed me to convert videos into readable content quickly, with minimal effort and cognitive load.\nthe responsibilities were cleanly split. i picked a video, copied the id, triggered the run, and occasionally checked whether it worked. the scripts handled the rest: transcript download, prompt formatting, output writing, and site generation.\nit felt like control.\nwhy scripts felt like the right tool at first scripts removed the tedious middle steps: download transcript → format prompt → save response. they gave me repeatability without committing to “architecture.”\nmore importantly, they kept the whole thing understandable.\nsame input. same process. predictable failure. if something broke, it broke loudly. if something was wrong, it was usually obvious. there was no hidden state and nothing to “maintain.” each run started fresh. if it didn’t work, i could throw it away and try again.\nand that mattered, because at the beginning, i wasn’t building a product. i was learning.\nthe limits of scripts i didn’t notice immediately the problem is that it feels harmless when scripts make certain assumptions.\ni assumed i would remember what i had processed. i assumed a file on disk was “state”. i assumed manual selection would stay sustainable. i assumed “just run it again” was a valid recovery strategy.\nthose assumptions don’t break immediately. they just quietly get more expensive.\nfiles had to be in exactly the right place. concurrent runs weren’t really a thing. “did i already do this?” lived in my head. idempotency was implied, not designed.\nand over time, the questions i had to answer became less and less script-shaped:\nwhy does this video have an output, but that one doesn’t? did this fail because the run failed—or because the video is weird? what happens if i run the same command twice? does my blog reflect everything i processed, or just the most recent success? at some point, the gap between “this works” and “this is correct” widened silently.\ni didn’t notice at first. i noticed later. and that’s always worse.\nthe moment i realized this was becoming a system then i discovered entire channels and playlists that were worth processing.\nsuddenly, it wasn’t “process this one video.” it was:\nprocess these 47 videos, track which ones matter, don’t redo work, remember what failed, and do it again next week when new videos show up.\nthat was the moment my brain became the bottleneck. i spent more time tracking what i had already done than actually reading the results. scripts assume ephemeral work. playlists assume persistence.\nrunning the script on video 23 of 47 meant i couldn’t even answer a basic question: what state is this whole batch in? files kept piling up with no index. manual tracking became the critical path. and i hated that feeling.\nthe pause moment came when i caught myself thinking:\ni should write another script to track what the first script processed.\nthat’s the moment scripts stop being tools and start becoming a system with a hidden state.\nwhat changed once state entered the picture once a state exists, you can’t pretend it doesn’t.\nthe questions changed:\nhas this video been processed? (not “does a file exist?”)\nis this video worth processing? (assessment becomes data, not just a one-time decision)\nwhen was this last updated? (staleness matters)\ndid this fail, or is it pending? (intent vs. outcome)\npersistence became unavoidable. subscription tracking. assessment scores. processing status (queued, in-progress, completed, failed). history.\nand new failure modes appeared—the annoying kind:\npartial batch completion with no resumption strategy.\nthe state split between filesystem and database is drifting out of sync.\n“successful” execution producing a corrupted or misleading state.\nscripts fail cleanly. systems fail partially.\nwhen you’re building something you trust, that difference matters.\nthe trap of “vibe-trash” to remove friction, i started experimenting with agentic frameworks like pydanticai and deveoped my own claude code commands (with templates) for spec driven development.\nand because python isn’t my native language, i didn’t immediately notice what was happening: the ai was producing code that worked today while quietly increasing the chaos tomorrow.\nthe coding assistant would create new files instead of editing an existing one—because “adding is easier than editing.” suddenly, i had a folder full of scripts with unclear purpose and ownership. things “worked,” but only in the fragile way that makes you afraid to touch anything.\ni started calling this vibe-trash: code that passes the vibe check now, and fails six decisions later.\nthe tooling also made assumptions i couldn’t always see in time:\na new setup means a new schema. running migrations is helpful. “this works now” equals “this is correct.” and of course: i can evaluate the consequences. i couldn’t.\nwhen you’re not fluent in the language, you don’t evaluate correctness—you evaluate whether it runs. helpful automation becomes dangerous automation.\nthe crash and the wipe the turning point wasn’t gentle. it was a crash.\nwhen anthropic introduced weekly limits and claude code would not for four days. i was stuck. i was furious. and in that frustration, i made a decision that was emotionally obvious and technically inevitable: i moved the project back to my roots — php and symfony.\nand then the worst thing happened.\nas claude code\u0026rsquo;s first act of “helping” set up the symfony environment, claude code deleted my entire database. it assumed “fresh start” was the right thing to do.\nit wiped assessments i had paid for with real openai tokens. it wiped decisions. it wiped history.\ni was angry at the ai. but mostly i was angry at myself.\nbecause i had no backup. i had no guardrails. i had confused “automation” with “safety.”\ni had to rebuild an import pipeline from scattered blog files. some things i could reconstruct. some i couldn’t. the pain wasn’t the work — it was the realization that i had built something where loss was possible, and done nothing to prevent it.\nwhat happened after the wipe the first thing i did after restoring what i could from scattered blog files was write a backup script. it runs every night. i check the logs every morning. i don\u0026rsquo;t fully trust it not to fail, so i check.\ni added a comment to every migration file: # human only - do not let ai run this. i tell claude this in every session. it still tries occasionally. i keep reminding the ai. the pattern is exhausting, but data loss is worse.\nthen i made a decision that changed the project more than i expected: i built an actual application with a ui.\nnot because the ui was hard—i do this professionally, so building a symfony app with controllers and views is familiar territory. but because i needed to see what was happening.\nscripts hide their state. an application exposes it.\nsuddenly, i could look at a screen and answer questions i couldn\u0026rsquo;t answer before:\nwhich videos are queued vs. in-progress vs. completed? what\u0026rsquo;s the assessment score for this video, and when was it calculated? which model did the assessment use, when, and how long did it take? what\u0026rsquo;s the processing history for this channel? the workflow became visible. data moving through the pipeline wasn\u0026rsquo;t a guess anymore—it was something i could observe, verify, and trust.\nwhat changed in how i think about this before the wipe, i thought the problem was: how do i process more videos faster?\nafter the wipe, i realized the problem was: how do i protect the decisions i\u0026rsquo;ve already made?\nthe assessments—scores that determine whether a video is worth processing—those cost real money. openai tokens aren\u0026rsquo;t free. re-running thousands of assessments because i didn\u0026rsquo;t think about backups isn\u0026rsquo;t \u0026ldquo;a learning experience.\u0026rdquo; it\u0026rsquo;s expensive stupidity.\nand that changed what i cared about:\ni stopped experimenting with where my data lives. experiments happen in throwaway environments now. the production database is not a playground.\ni started treating state transitions as events that need approval. not automation that \u0026ldquo;just runs.\u0026rdquo; if something is going to change the state of 500 videos, i want to see the plan first.\ni realized the application isn\u0026rsquo;t the clever part—it\u0026rsquo;s the boring part that makes the clever parts survivable. symfony isn\u0026rsquo;t exciting. but doctrine transactions, database constraints, and a ui that shows me what\u0026rsquo;s actually happening? those make the difference between \u0026ldquo;this might work\u0026rdquo; and \u0026ldquo;i can trust this.\u0026rdquo;\nthe tools i added the backup script checks the database nightly and creates snapshots. it\u0026rsquo;s not sophisticated. it just works.\nthe ui has a status dashboard. i can see the processing state at a glance. no more wondering \u0026ldquo;did that finish?\u0026rdquo; or \u0026ldquo;which videos failed?\u0026rdquo;\nthe application enforces a separation: assessment happens in one place, processing happens in another. they\u0026rsquo;re decoupled. if assessment breaks, processing still works with old scores. if processing breaks, i\u0026rsquo;m not re-running expensive assessments.\nand i started writing tools that exist solely to monitor the other tools. a script that checks whether the backup ran. a view that shows which videos have been queued for more than 24 hours. a report that flags videos where assessment and processing state don\u0026rsquo;t match.\nwhen you start building tools to watch your tools, you\u0026rsquo;ve crossed a line. you\u0026rsquo;re no longer scripting. you\u0026rsquo;re operating a system.\nboundaries and non-goals scripts are still great for exploratory work. this isn\u0026rsquo;t a critique of scripts.\nit\u0026rsquo;s also not a critique of vibe-coding, but you need to be able to evaluate what gets produced. when you can\u0026rsquo;t tell if the code will break tomorrow, helpful automation becomes dangerous.\nthe problem isn\u0026rsquo;t scripts. it\u0026rsquo;s using script-thinking when system-thinking is required.\nwhat i\u0026rsquo;d do differently next time i wouldn\u0026rsquo;t wait for a database wipe to add backups. that\u0026rsquo;s the obvious one.\nbut more importantly, i\u0026rsquo;d recognize the moment when i started wanting to \u0026ldquo;track just one more thing\u0026rdquo; as the signal that state has entered the picture. that\u0026rsquo;s when scripts stop being the right tool.\nthe question i ask now when i\u0026rsquo;m tempted to add tracking to a script: is this metadata, or is this state?\nif it\u0026rsquo;s a state, the script is lying to me. it\u0026rsquo;s pretending to be stateless while depending on something that isn\u0026rsquo;t.\nthe signal you\u0026rsquo;ve built a system the clearest indicator isn\u0026rsquo;t architectural. it\u0026rsquo;s behavioral.\nyou start writing tools that monitor your other tools.\nyou start checking whether things happened, not just whether they succeeded.\nyou start caring about the order of operations, not just their completion.\nyou realize \u0026ldquo;just run it again\u0026rdquo; isn\u0026rsquo;t a recovery strategy anymore—it\u0026rsquo;s a way to make things worse.\nat that point, you\u0026rsquo;re not migrating from scripts to a system. you\u0026rsquo;re admitting you\u0026rsquo;ve been building a system with script-shaped tools, and adjusting accordingly.\n","title":"When Scripts Turn into a System"},{"date":"2026-01-17","image":"https://blog.dazzlog.de/posts/2026-01-17_application-layer-ai-engineering//hero.png","imageAlt":"","link":"https://blog.dazzlog.de/posts/2026-01-17_application-layer-ai-engineering/","summary":"\u003cp\u003eI have two kids and no time to watch hours of YouTube tutorials. Also, I find the information density very sparse and extracting the information very time-consuming. My \u003cstrong\u003e\u0026ldquo;Neo from the Matrix\u0026rdquo; fantasy\u003c/strong\u003e has always been to have knowledge injected directly into my brain—skipping the medium and going straight to the data. So my wish formulated that I would like to have the information extracted, so I can expose my brain to the compacted knowledge and speed through all the knowledge I would like my brain to have consumed.\u003c/p\u003e","tags":["ai","ai engineering","application","application layer","workflow"],"text":"i have two kids and no time to watch hours of youtube tutorials. also, i find the information density very sparse and extracting the information very time-consuming. my \u0026ldquo;neo from the matrix\u0026rdquo; fantasy has always been to have knowledge injected directly into my brain—skipping the medium and going straight to the data. so my wish formulated that i would like to have the information extracted, so i can expose my brain to the compacted knowledge and speed through all the knowledge i would like my brain to have consumed.\nlast summer, i decided to build my way out of that friction.\nthe initial win and the illusion my plan was straightforward and, at the time, i didn\u0026rsquo;t even think of it as an \u0026ldquo;application\u0026rdquo;. i just wanted to feed video transcripts to an ai, extract the information into clean markdown, and push those files to a git directory. from there, a static site generator and some github actions would turn the data into a readable blog post. i wanted something i could consume via a browser or rss feed, fully replacing the need to watch the video.\nthe first results were exceptionally good—the ai captured structure and nuance far better than i expected. for a brief moment, it felt like i had solved the problem with just a few clever prompts.\nwhere reality disagreed the friction started almost immediately, and it wasn’t an ai problem—it was a manual effort problem. i found myself trapped in a boring, repetitive cycle of copying video ids into a makefile and running python scripts manually. i was playing \u0026ldquo;human cron job\u0026rdquo;.\nas i processed more content, the markdown files started piling up. managing the metadata, searching through the directory, and deduplicating entries became significantly harder than generating the content itself. that was the moment i realized that a database wasn\u0026rsquo;t just a \u0026ldquo;nice-to-have\u0026rdquo; feature; it had to be the center of the project.\nfrom script to system the turning point was realizing that i didn\u0026rsquo;t want a better script or better prompts; i wanted an agentic system that could discover, ingest, analyze, and publish content without my intervention. i had to move beyond \u0026ldquo;generating text\u0026rdquo; and start \u0026ldquo;operating a workflow\u0026rdquo;. this forced me to shift my thinking toward maintainability and long-term stability.\nwhile i started with python-based frameworks, i quickly hit a wall of \u0026ldquo;vibe-trash\u0026rdquo;—a pile of scattered scripts that were easy to write but impossible to reason about weeks later. i eventually moved the entire project to php and symfony. moving to my \u0026ldquo;native\u0026rdquo; environment allowed me to own the orchestration layer and build something i could actually debug when things went wrong.\n\u0026#xf400; i learned i have to be the expert evaluating the output of the ai. otherwise, the output it will be vibe-trash. to ensure the system remained safe and queryable, i moved away from ephemeral files toward a more robust stack: postgresql with pgvector for embeddings and neo4j for knowledge graphs. this wasn’t about over-engineering; it was about building a foundation that could handle the consequences of state, consistency, and scale.\n\u0026#xe756; symfony/ai symfony ai is a set of components that integrate ai capabilities into php applications, providing a unified interface to work with various ai platforms like openai, anthropic, google gemini, azure, and more.\nsee https://symfony.com/doc/current/ai/index.html\nengineering over magic here\u0026rsquo;s what became clear: \u0026ldquo;knowledge extraction\u0026rdquo; sounds like a single ai task. it\u0026rsquo;s not. it\u0026rsquo;s a pipeline with distinct stages:\ngather – fetch transcripts, manage metadata normalize – chunk at semantic boundaries, disfluency removal transform – extract entities and relationships store – vectors and graphs retrieve – routed search methods most of these stages have nothing to do with ai.\nthey are fundamental, necessary steps. i stopped looking for the \u0026ldquo;perfect model\u0026rdquo; and started building a transcriptchunker to preserve timestamps and a database schema that could actually handle metadata.\nwhat surprised me was that ai is most effective when it is given the smallest possible job. i found that chaining specific, isolated ai calls produced far more reliable results than one massive \u0026ldquo;do everything\u0026rdquo; request.\n\u0026#xf400; i learned if the input data is messy or the early steps in my pipeline are flawed, no amount of clever prompt engineering can save the output downstream. the realization: plumbing over prompts ai fails silently and gracefully. without proper tracing, i couldn’t tell the difference between an output that \u0026ldquo;looked good\u0026rdquo; and one that actually captured the knowledge i needed. i saw that if i couldn\u0026rsquo;t trace exactly how a piece of knowledge was extracted, i didn\u0026rsquo;t have a system—i just had a demo.\nthe architecture surrounding the ai—the database design, the way i decided to split chunks of text, the preservation of metadata—is where the real engineering happens. these are application problems, and they are where most ai projects quietly fall apart.\ni am still figuring out the optimal patterns for this. structured guidance is scattered across half-finished repos and old blog posts, and i’m still learning how to search for the \u0026ldquo;right\u0026rdquo; way to build these systems. but my focus has permanently shifted: i care less about individual prompts and more about the traceable, repeatable system that makes those prompts work.\nthat is the work that actually scales—and the only way i\u0026rsquo;ll ever get that \u0026ldquo;neo\u0026rdquo; style knowledge injection.\n","title":"I Thought I Needed Better Prompts. I Needed a System."},{"date":"2025-08-17","image":"https://blog.dazzlog.de/posts/2025-08-17_summer-projects//hero.png","imageAlt":"","link":"https://blog.dazzlog.de/posts/2025-08-17_summer-projects/","summary":"\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;Three weeks, three websites — and yes, I actually took a proper holiday too.\u0026rdquo;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eDuring my summer vacation 2025, I had a bit of time each evening to hack away at my computer. Since I\u0026rsquo;d been wanting to tackle these projects for a while and felt motivated, I decided to take them on one after another. It wasn\u0026rsquo;t originally planned as \u0026ldquo;one per week\u0026rdquo;, but that\u0026rsquo;s the rhythm that naturally emerged from the time I had available, the pace of development, and how long each project actually took. I\u0026rsquo;ll get to what these projects were in a moment, but what I found more important was my goal to develop a solid workflow with \u003cstrong\u003eClaude Code\u003c/strong\u003e.\u003c/p\u003e","tags":["ai","bephpug","bulma","c-base","claude code","css","tailwindcss","workflow","zola"],"text":" \u0026ldquo;three weeks, three websites — and yes, i actually took a proper holiday too.\u0026rdquo;\nduring my summer vacation 2025, i had a bit of time each evening to hack away at my computer. since i\u0026rsquo;d been wanting to tackle these projects for a while and felt motivated, i decided to take them on one after another. it wasn\u0026rsquo;t originally planned as \u0026ldquo;one per week\u0026rdquo;, but that\u0026rsquo;s the rhythm that naturally emerged from the time i had available, the pace of development, and how long each project actually took. i\u0026rsquo;ll get to what these projects were in a moment, but what i found more important was my goal to develop a solid workflow with claude code.\ni\u0026rsquo;ve been exploring claude code for a few weeks now, and the topics you can research and discover with it are numerous.\nclaude code features \u0026#xf05a; claude code claude code is an agentic coding tool from anthropic that runs directly in your terminal, allowing you to efficiently delegate programming tasks using natural language — without any extra chat windows or ide integration.\nlink: https://docs.anthropic.com/en/docs/claude-code/\nslash commands /commands \u0026#xf05a; claude code slash commands custom slash commands allow you to define frequently-used prompts as markdown files that claude code can execute. commands are organized by scope (project-specific or personal) and support namespacing through directory structures. link: https://docs.anthropic.com/en/docs/claude-code/slash-commands the generate-command is my absolute favourite from my current collection, because it lets you create infinitely more and specialized commands.\n.claude/commands/generate-command.md\n--- allowed-tools: read, write, bash(mkdir:*), bash(pwd) description: add a reusable slash command so anyone who clones your repo can invoke it with `/project:\u0026lt;name\u0026gt;` inside the claude code cli. --- ## context - current directory: !`pwd` - current date: !`date +%y-%m-%d` ## usage ``` /generate-command analyze content for seo optimization opportunities across the blog ``` ... examples of commands for my technical blog\naudit-reading-time.md audit-seo.md post-scaffold.md posts-validate-content.md team-add-member.md sometimes you don\u0026rsquo;t even realize what you could automate, but when you work on a project for a while and start bundling work steps into commands, you notice more and more things that can be automated through commands.\nsubagents /agents \u0026#xf05a; claude code subagents subagents are preconfigured ai personalities that claude code can delegate tasks to. each subagent operates in its own context, preventing contamination of the main conversation and keeping it focused on high-level goals.\nlink: https://docs.anthropic.com/en/docs/claude-code/sub-agents\nproject level: .claude/agents/ user level: ~/.claude/agents/ --- name: subagent-name description: description of when this subagent should be called tools: tool1, tool2, tool3 # optional --- your system prompt for the subagent... when you have an idea for a subagent, you can have claude generate one for you using /agent, which will then be stored in .claude/agents/.\nbackground jobs /bashes claude code can also run background jobs that aren\u0026rsquo;t directly connected to the current conversation. this is practical for development with a static site generator, as you can have claude monitor the build process logs and react directly when errors occur.\nwhy zola? i wanted to create a technical blog. previously my blog ran on jekyll, now on hugo, but i wanted to see what was currently popular before just defaulting to hugo. the decision really came down to hugo or zola, but then i noticed that zola uses jinja2-like syntax for templates. i know jinja2-style syntax from the symfony world as twig, and i can understand it easily — plus anyone who might want to extend the blog in the future (my colleagues also know twig) can understand it too. that was an important decision criterion for me.\nstatic site generation means the website is generated upfront and then served as static html files. this has performance and security advantages since no database queries or server-side logic are needed. github has been offering github pages for this for years, and it\u0026rsquo;s incredibly simple to host a static website.\n\u0026#xf05a; zola zola is a static site generator (ssg), similar to hugo, pelican, and jekyll. it is written in rust and uses the tera template engine, which is similar to jinja2, django templates, liquid, and twig.\nlink: https://www.getzola.org/\nproject 1: technical blog for open software consulting \u0026#xf188; not online yet this page needs to be discussed with some decision makers before it gets deployed. stay tuned. since there was no technical blog before, i could start completely fresh without any legacy concerns. i wasn\u0026rsquo;t familiar with zola yet and how it works, so i decided after a bit of research on the deepthought theme, which i liked and also provided a good foundation for open software consulting\u0026rsquo;s corporate identity.\ni gave claude the brand colors and that\u0026rsquo;s how the current design of the site emerged. my workflow with claude code involved creating a plan.md file where i listed all the tasks i wanted to complete. i then kept updating this file and prompted claude to take a task from the file, work on it, read the file (in case it had changed), check it off, commit, and then take the next task. this helped me keep track of everything and also document the work steps.\n\u0026#xf05a; deepthought a simple blog theme focused on writing powered by bulma and zola\nlink: https://deepthought-theme.netlify.app/\nreading time something i hadn\u0026rsquo;t thought of at all was reading time for blog posts. this is a feature claude suggested and i really like it. claude helped me calculate the reading time for each blog post and store it in the frontmatter. this was super practical because i could then display the reading time directly in the blog posts. there\u0026rsquo;s now also a claude code command /project:audit-reading-time that calculates reading time for all blog posts and stores it in the frontmatter.\nteam/authors since i created the site for open software consulting, i also wanted to include team members as authors. for this, i created a /project:team-add-member command that allows me to add team members and create their profiles. this is super practical because i can easily manage team members and update their profiles. under /team/ you can find the team members and their profiles, which i created with claude code. each author also has a list of their blog posts that they\u0026rsquo;ve written.\nevents i also wanted to include the symfony user group events that we\u0026rsquo;ve been organizing for years. for this, i created a /project:events-add-event command that allows me to add events and manage their details. the events are displayed under /events/ and are organized in the same structure as blog posts. this makes it easy to manage events and display them on the website.\ntaxonomies i learned these are super important for structuring content. i created taxonomies for authors, categories, and tags to better organize the blog posts. zola does a fantastic job making tags, categories, and authors easily browsable.\nsearch zola has a built-in search function that i activated for the blog. this is super practical because readers can quickly find the content they\u0026rsquo;re looking for. i also created a /project:posts-validate-content command that ensures all blog posts are correctly formatted and contain no errors.\nadditional features cookie banner: implemented to comply with privacy regulations. admonition macro: for special notices in blog posts. responsive design: the design is mobile-friendly and adapts to different screen sizes. hero images: each blog post has a hero image defined in the frontmatter. taxonomy pages: pages that display all blog posts and events by categories and tags. seo optimization: blog posts are optimized for search engines to ensure better visibility. accessibility: the website is designed to be accessible to all users. performance optimization: the website is fast and responsive since it\u0026rsquo;s statically generated. deployment: the website is automatically deployed to github pages when code changes are made. content validation: a command that ensures all content is correctly formatted and contains no errors. markdown support: all content is written in markdown, making editing and maintenance easier. syntax highlighting: code snippets in blog posts are automatically highlighted to improve readability (without line numbers). table of contents (toc): automatically generated table of contents for blog posts that enable better navigation. summary i was amazed that i was done with the site after 5 days. and that\u0026rsquo;s even though i only had a few hours each evening to work on it. i learned a lot about zola and how taxonomies work. i also learned how easy it is with claude code to quickly get a simple project up and running. i hadn\u0026rsquo;t known bulma css before, but it\u0026rsquo;s a lightweight css framework that works well with zola and helped me quickly create an appealing design.\n\u0026#xf05a; bulma \u0026#xf08a; bulma is a free, open source, modern, lightweight, css-only framework that provides ready-to-use frontend components, based on flexbox for fast, responsive layouts with simple class names and no javascript.\nlink: https://bulma.io/\nin four days, this project went from zero to:\n67 commits (averaging 16 commits per day) 2 blog posts with comprehensive content 5 event pages with real community data 1 team member profile with avatar and expertise (mine) complete taxonomy system for content organization advanced ui components including admonitions and toc project 2: c-base artefakt-guide the c-base artifact guide is a guide to the discovered artifacts of c-base for members of c-base, the crashed space station under berlin-mitte. the relaunch was necessary because the old website was outdated and no longer met requirements. the connection to c-base\u0026rsquo;s ldap system no longer worked and the php libraries were so outdated that they could no longer be updated. a website relaunch i had planned was not completed twice, so i decided to recreate the website with zola.\nwhat\u0026rsquo;s missing now is the editing functionality for members, but i consider that a feature that can be added later. the website is now static and maintained by c-base members by updating the markdown files in the repository. i did a bit of research - there are even uis for sites stored in git that allow content editing, but i decided not to set that up for now because it makes things technically more complex again and the project should first be relaunched in a simple version. one challenge is that the urls must remain the same, since we have qr codes hanging on board that scan to the artifact guide for specific artifacts.\nthis time too, i created an initial document with claude code where i described in great detail what i wanted to do. the goal, current status, and desired end state. i already knew i wanted to use zola, but for this project the layout was so specific that a ready-made theme would have mostly gotten in the way. after brief research, i decided on a custom theme. with tailwindcss and daisyui, because both are on my list of things i want to gain experience with. this would later give me headaches and delay the project.\none requirement i set for every project is that they don\u0026rsquo;t depend on any external sources in production. no cdns for fonts, css, or javascript libraries. all resources are delivered statically with the site. but i still started with tailwindcss delivered via cdn and postponed the problem of not finding a proper way to include it until later.\n\u0026#xf057; tailwindcss needs npm or cdn tailwind needs a javascript (npm) build step to parse existing css for used classes and save them in a css file. either the javascript comes via cdn that parses my site at runtime (and tracks all users), or i install npm and the javascript libraries to parse locally. for me, this mainly means that tailwindcss becomes unusable, because:\nmy deploy process should be as simple as possible npm is hell and i won\u0026rsquo;t voluntarily let it into my house. it opens a dependency hell that becomes outdated and opens security vulnerabilities as soon as you don\u0026rsquo;t update for 5 minutes. end of story. all the layout had to be recreated from scratch. once that was clear, i gave claude the task of migrating the existing layout to bulma (which we already know). that took a few minutes until everything fit, and i had to help out quite a bit, and after running into the usage limit at least twice, the layout was sleek, i liked it, and it wasn\u0026rsquo;t dependent on anything or anyone anymore.\nthis project contains a long list of features:\nmultilingual with c-lang \u0026#xf259; c-lang c-lang is the constructed language of the c-base space station. the escalation system (0-3) provides increasing complexity from basic member communication to advanced c-lang usage. since c-lang isn\u0026rsquo;t iso standard, normal multilingual features can\u0026rsquo;t handle it, and it took a while until i found an approach that wasn\u0026rsquo;t cumbersome, didn\u0026rsquo;t require writing markup in markdown, or put all texts in the toml configuration. since most multilingual features change the url per language, that wasn\u0026rsquo;t an option, because the url had to be fixed due to the qr codes pointing to them.\nso now it\u0026rsquo;s solved by having a separate file for each language and making the switch in the template. the main configuration is done in the default language, german and english only adjust the title.\ntaxonomy for decc, location and ring now you can browse the list of artifacts from the oberdecc or the culture ring. this also generates the next artifact that invites further browsing.\nimage credits and image carousel image credits are preserved. the images were previously stored in a database and linked to artifacts with an n:m relation. without a database, this now becomes denormalized into a 1:n relation: each artifact has many images. the images are entered in the article\u0026rsquo;s toml config. the number of images determines whether a carousel is shown or just the image without control elements and pagination.\ndeployment the project is packaged into a docker image via github actions and pushed to github\u0026rsquo;s container registry tagged as latest. then a webhook is called that ensures a script on a c-base server pulls the image and starts a container from it. wasn\u0026rsquo;t a problem at all, but something new to learn from github.\nsummary i would have liked to avoid the tailwindcss route, but i\u0026rsquo;m glad i went through it because now i don\u0026rsquo;t have to make the same journey in my project at work and already know that bulma is the alternative.\nclaude helped with:\ncreating the custom layout, twice the trilingual language switch without ready-made language features content migration of existing artifacts and their images scripts and commands to check: valid frontmatter valid taxonomies all registered images are present and all existing images are registered all links work all translations are present detailed documentation ci/cd with github actions the project was super fun and quite a bit more time needs to be invested to claim the artifact guide is finished, because with this project, content is king. the site is a feast for the eyes and does justice to the space station. a good moment for a relaunch, because the space station turns 30 this year!\n\u0026#xf259; c-base c-base celebrates its 30th orbit around the sun on earth this year. in 6 days, this project went from zero to:\n64 commits (averaging 10 commits per day) 92 c-base artifacts with complete metadata and imagery 279 multilingual content files across c-lang, german, and english 106 artifact images co-located with content for optimal organization 3 complete language versions with client-side switching for qr-code compatibility 7 comprehensive validation scripts ensuring content quality and consistency 1,596 lines of custom zola templates built from scratch for authentic c-base styling complete taxonomy system covering decc classification, station locations, and ring mapping full docker deployment pipeline with multi-stage builds and nginx configuration advanced search functionality across all languages and artifact metadata production-ready static site serving authentic c-base space station culture project 3: relaunch berliner php usergroup website the berlin php user group site was created in 2011, before that it was a wiki (?). jekyll was used, which was state-of-the-art at the time. the site has, as mentioned, gotten long in the tooth and i had no desire to maintain an old site after taking over the organization of the berlin php usergroup.\n\u0026#xf05a; jekyll jekyll is a static site generator. it takes text written in your favorite markup language and uses layouts to create a static website. it\u0026rsquo;s written in ruby and was one of the first ssgs that you could use with github pages.\nlink: https://jekyllrb.com/\ni analyzed the content of the old site and found that it would be better to represent the \u0026ldquo;\u0026ldquo;archive\u0026rdquo; as the list of events, which is what the usergroup is all about, since a markdown page was diligently created each time. many events also include slides and source files that speakers had uploaded. i wasn\u0026rsquo;t sure at first whether we should keep them, but after seeing that it was under 100mb, the question was off the table and the answer was a clear \u0026ldquo;yes!\u0026rdquo;\ntogether with claude, i created a spec until everything i envisioned was correctly listed:\nzola ssg with custom theme using bulma css php colors landing page list of all events taxonomy for speakers and topics i first imported one event, checked that everything fit, then we imported 5 more events from the old repository and more adjustments had to be made. then i had to take a break for the day because i hit claude\u0026rsquo;s token limit. after that i just had claude to its thing importing all events and deciding on the topics for each event. i would never have tried to do this if it wasn\u0026rsquo;t for a helping hand from ai.\nevents thanks to the simple structure, you can now easily view all past events and browse the topics and speakers for each event.\nspeaker each speaker has their own page listing all events where they gave a presentation. list of all speakers in the overview of all speakers, speakers are sorted so that those with the most talks are listed at the top. summary i\u0026rsquo;m excited that with the new site there\u0026rsquo;s now a way to browse the long history of the berlin php usergroup. i thought it was cool to see the numbers:\n\u0026#xe73d; berlin php usergroup ~13 years 98 meetups 103 speakers link: https://bephpug.de/\nthat really motivates me!\nthe project evolved from initial setup to a fully functional community website with automated deployment in just 4 days\n14 commits across 4 development days complete static site generator migration from jekyll to zola complete website redesign with bulma css events system with modern styling and navigation seo-friendly url structure for all events historical event archive with 98 migrated events modern taxonomy system for speakers and topics production deployment pipeline with github actions what i learned from all 3 projects why \u0026ldquo;staying out of the way\u0026rdquo; is an underestimated feature: zola\u0026rsquo;s simplicity meant i spent time solving actual problems rather than fighting the tool. when technology doesn\u0026rsquo;t create friction, you can focus on what matters - the content and user experience.\nbare minimum first: there are a ton of features that can be added, but they all make the project more complex and introduce possible bugs. keeping it simple and doing only the bare minimum to have the project shippable was a fun achievement.\nhow different projects help you really understand a tool: each project pushed different boundaries:\nproject 1 taught me zola basics and taxonomies project 2 forced me into custom themes and multilingual complexity project 3 showed me content migration and community site patterns tips for other developers wanting to start with zola:\nstart with an existing theme for your first project to understand the patterns embrace the simplicity - don\u0026rsquo;t over-engineer from the start use taxonomies liberally - they\u0026rsquo;re powerful for content organization co-locate assets with content when possible the template system is intuitive if you know jinja2/twig summary \u0026amp; outlook quick retrospective: 3 websites – check.\nkey insight: a good tool is one that allows you to implement your ideas quickly.\nthree weeks of evening development resulted in three production (ready) websites, each serving real communities and solving actual problems. the combination of claude code\u0026rsquo;s intelligent assistance with zola\u0026rsquo;s straightforward architecture created a development experience that felt more like guided conversation than traditional programming.\nthe pattern that emerged wasn\u0026rsquo;t planned but proved effective: detailed planning documents, iterative development with ai assistance, and focus on shipping functional solutions over perfect implementations. sometimes the best workflow is the one that naturally evolves from the constraints you\u0026rsquo;re working within.\n","title":"Summer Projects 2025: 3 websites in 3 weeks"},{"date":"2025-03-26","image":"https://blog.dazzlog.de/posts/2025-03-26_running-php-in-a-multi-process-container//multi-process-container.webp","imageAlt":"","link":"https://blog.dazzlog.de/posts/2025-03-26_running-php-in-a-multi-process-container/","summary":"\u003ch2 id=\"the-dockerized-development-setup\"\u003eThe Dockerized Development Setup\u003c/h2\u003e\n\u003cp\u003eContainerization has completely changed how we build and deploy PHP applications. With Docker, you can make sure that your production environment behaves just like your local setup, which means fewer surprises when you go live.\u003c/p\u003e\n\u003cp\u003eIn this post, we\u0026rsquo;re diving into running Symfony in a container that runs multiple processes using s6-overlay. We\u0026rsquo;ll explain why having more than one process in a container can be important, how this idea is different from Docker\u0026rsquo;s usual “one process per container” rule, and how s6-overlay makes it easier to run everything together.\u003c/p\u003e","tags":["ci","devops","docker","s6-overlay"],"text":"the dockerized development setup containerization has completely changed how we build and deploy php applications. with docker, you can make sure that your production environment behaves just like your local setup, which means fewer surprises when you go live.\nin this post, we\u0026rsquo;re diving into running symfony in a container that runs multiple processes using s6-overlay. we\u0026rsquo;ll explain why having more than one process in a container can be important, how this idea is different from docker\u0026rsquo;s usual “one process per container” rule, and how s6-overlay makes it easier to run everything together.\nwhether you\u0026rsquo;re new to s6-overlay or looking to improve your container setup, this guide walks you through practical, step-by-step tips to help you run your symfony apps more smoothly. enjoy discovering a simpler, more flexible way to work with containers!\nwhy two containers? typically, a dockerized php development environment consists of:\nphp-fpm container: runs the php application. nginx container: serves static files and proxies requests to php-fpm. and the docker-compose.yaml looks like this:\n# docker-compose.yaml services: app: image: php:8.4.1-fpm-alpine volumes: - ./app:/var/www/html nginx: image: nginx:latest ports: - \u0026#34;8000:80\u0026#34; volumes: - ./app/public:/var/www/html/public depends_on: - app this setup is common, it comes because of two primary reasons:\nlack of a built-in web server:\nphp does not include a production-grade http server. unlike languages like go or ruby, php relies on external servers (nginx/apache) to manage http requests.\ndocker’s “one process per container” mantra:\ndocker traditionally encourages running a single process per container. the reason for that i will explain a bit further down. this leads to separate containers for each service, complicating inter-process communication and health monitoring.\nsequencediagram participant client participant nginx participant php-fpm client-\u003e\u003enginx: send http request (e.g., /index.php) nginx-\u003e\u003ephp-fpm: forward request via fastcgi php-fpm-\u003e\u003ephp-fpm: process php script php-fpm-\u003e\u003enginx: return output (html, json, etc.) nginx-\u003e\u003eclient: send http response why is it a problem running two containers? running multiple containers isn\u0026rsquo;t inherently problematic. some hosting platforms limit multi-container deployments, pushing developers toward single-container solutions. but this isn\u0026rsquo;t a technical limitation — it\u0026rsquo;s often a constraint of hosting.\nbuild-time dependency between containers when your php application generates static assets that your webserver needs to serve, you\u0026rsquo;re essentially creating a build-time dependency between containers. this isn\u0026rsquo;t automatically bad, but it reveals potential architectural weaknesses.\nthe core problem isn\u0026rsquo;t multiple containers—it\u0026rsquo;s mixing concerns. static assets should be treated as build artifacts, not runtime-generated content. containers should be immutable; writing files during runtime contradicts container best practices.\novercoming the one process per container constraint understanding container isolation \u0026#xf259; tldr; in docker containers, the first process (pid 1) is responsible for handling system signals and managing child processes. if pid 1 doesn\u0026rsquo;t properly handle termination signals like sigterm, it can lead to issues such as zombie processes—completed processes that remain in the process table, potentially causing resource exhaustion. containers provide isolation by encapsulating an application\u0026rsquo;s filesystem, networking, and process tree. in docker, the first process started within a container is assigned process id 1 (pid 1). this process becomes the init process for the container, responsible for handling system signals and managing child processes.\ndocker relies on sending signals like sigterm and sigkill to the container\u0026rsquo;s pid 1 to manage lifecycle events such as stopping or restarting the container. however, if the process running as pid 1 isn\u0026rsquo;t designed to handle these signals properly, it may not terminate gracefully upon receiving a termination signal. this can lead to issues like zombie processes — processes that have completed execution but still have an entry in the process table because their parent hasn\u0026rsquo;t acknowledged their termination. zombie processes can accumulate over time, leading to resource exhaustion and degraded system performance.\nto mitigate these issues, it\u0026rsquo;s important to ensure that the process running as pid 1 in your container can handle system signals appropriately and manage child processes effectively. one approach is to use a minimal init system or an init-like process as pid 1. these init systems are designed to forward signals to child processes and reap zombie processes, ensuring proper process management within the container. for instance, using the exec command in shell scripts can replace the shell process with the intended application process, ensuring it becomes pid 1 and can handle signals directly.\nalternatively, docker provides the --init flag, which runs an init process as pid 1. this init process is responsible for forwarding signals and reaping zombie processes, thereby improving the container\u0026rsquo;s process management.\nby addressing the pid 1 signal handling and zombie reaping issues, you can ensure that your containerized applications are more robust, responsive to lifecycle events, and free from resource leaks caused by lingering zombie processes.\nsequencediagram participant host as docker host participant container as docker container participant pid1 as pid 1 process participant child as child process host-\u003e\u003econtainer: docker stop app container-\u003e\u003epid1: sigterm alt pid1 handles sigterm pid1-\u003e\u003echild: forward sigterm child-\u003e\u003epid1: termination acknowledgment pid1-\u003e\u003econtainer: exit else pid1 does not handle sigterm note right of pid1: child processes may become zombies host-\u003e\u003econtainer: sigkill after timeout container-\u003e\u003epid1: sigkill pid1-\u003e\u003echild: terminate abruptly end what you see here is that when you execute the docker stop command, docker initiates a graceful shutdown process for the specified container. it first sends the sigterm signal to the main process inside the container, allowing the application to perform necessary cleanup operations. docker then waits for a default grace period of 10 seconds. if the process does not terminate within this timeframe, docker sends the sigkill signal to forcefully stop the container.\nthe role of s6-overlay \u0026#xf15c; s6-overlay s6-overlay is an easy-to-install (just extract a tarball or two!) set of scripts and utilities allowing you to use existing docker images while using s6 as a pid 1 for your container and process supervisor for your services.\nhttps://github.com/just-containers/s6-overlay\neasy integration: seamlessly integrate s6-overlay into docker images with a straightforward installation process. just extract a tarball or two! proper pid 1 functionality: it ensures that all child processes are managed and that signals are handled gracefully. you\u0026rsquo;ll never have zombie processes hanging around in your container, they will be properly cleaned up. versatile process management: s6-overlay efficiently handles both one-time tasks and long-running processes, making it versatile for containerized tasks. dependency control: establish dependencies between processes to ensure orderly execution in complex application stacks. sequence management: control the start and stop sequence of processes, streamlining container operations. environment variable templating: easily customize process behavior with environment variables, adapting to different environments. log management: built-in log rotation simplifies log file management within container environments. the supervision system automatically maintains an open pipe between the producer\u0026rsquo;s stdout and the logger\u0026rsquo;s stdin. graceful shutdown: ensure data integrity with graceful process shutdown and the ability to execute custom scripts before container shutdown. multi-arch support: s6-overlay accommodates the diverse landscape of container platforms with support for multi-architecture container images. setting up s6-overlay installation the installation of s6-overlay is straightforward. in your dockerfile, you typically add and extract two tarballs (one for noarch and one for your architecture). for example:\nfrom busybox arg release_path=\u0026#34;https://github.com/just-containers/s6-overlay/releases/download/v3.2.0.2\u0026#34; add $release_path/s6-overlay-noarch.tar.xz /tmp run tar -c / -jxpf /tmp/s6-overlay-noarch.tar.xz add $release_path/s6-overlay-x86_64.tar.xz /tmp run tar -c / -jxpf /tmp/s6-overlay-x86_64.tar.xz entrypoint [\u0026#34;/init\u0026#34;] here, /init becomes the container’s entrypoint and will be responsible for process supervision.\n(if you want to see a simpler setup jump to s6-overlay-base image)\nbasic usage with s6-overlay installed, your container’s command can be defined in two ways:\n\u0026#xf05a; using entrypoint with `/init` this enables s6-overlay to take over process supervision right from startup. \u0026#xf05a; using `cmd` for your application you can set your command (e.g., running php scripts) with cmd. this allows you to override the default behavior while still benefiting from s6-overlay’s supervision. for example:\nentrypoint [\u0026#34;/init\u0026#34;] cmd [\u0026#34;php\u0026#34;, \u0026#34;bin/console\u0026#34;, \u0026#34;messenger:consume\u0026#34;, \u0026#34;scheduler_default\u0026#34;, \u0026#34;--time-limit=300\u0026#34;] in this configuration, even if you override the command, s6-overlay will continue to manage your process lifecycle.\nservice types in s6-overlay s6-overlay supports three service types that let you control how processes run within your container:\noneshot: runs once and exits (e.g., initialization tasks). longrun: supervised by s6 (e.g., nginx or php-fpm). bundle: groups related services together. let\u0026rsquo;s look into all of them next:\n1. oneshot purpose: run a task once (e.g., initialization or migration scripts). configuration files: type (contains “oneshot”) up (path to the script) behavior: the service runs, completes its task, and then exits. 2. longrun purpose: manage long-running processes (daemons such as php-fpm or nginx). configuration files: type (contains “longrun”) run (executable command/script) behavior: s6-overlay supervises these processes and automatically restarts them if they exit unexpectedly. 3. bundle purpose: group related services so they can be started or stopped together. configuration files: type (contains “bundle”) contents.d (directory listing the grouped services) behavior: bundles allow you to manage multiple services as a single unit. a step-by-step example: running php-fpm and nginx 1. directory structure create the service root directory:\nmkdir -p /etc/s6-overlay/s6-rc.d then add the user bundle, which s6-overlay expects and uses as the entry point for all service configurations:\n/etc/s6-overlay/s6-rc.d └── user ├── contents.d # directory for service files └── type # file contains string \u0026#34;bundle\u0026#34; 2. defining the php-fpm service create the php-fpm service configuration:\n/etc/s6-overlay/s6-rc.d ├── svc-php-fpm │ ├── run │ └── type # contains \u0026#34;longrun\u0026#34; └── user └── contents.d └── svc-php-fpm and in /etc/s6-overlay/s6-rc.d/svc-php-fpm/run:\n#!/command/execlineb -p /usr/local/sbin/php-fpm --nodaemonize 3. defining the nginx service set up the nginx service with a dependency on php-fpm:\n/etc/s6-overlay/s6-rc.d ├── svc-nginx │ ├── dependencies.d │ │ └── svc-php-fpm │ ├── run │ └── type # contains \u0026#34;longrun\u0026#34; └── user └── contents.d └── svc-nginx and the content of /etc/s6-overlay/s6-rc.d/svc-nginx/run:\n#!/command/execlineb -p nginx -g \u0026#34;daemon off;\u0026#34; this ensures that nginx only starts after php-fpm is running.\n4. running the container once the configuration is in place, launch your container:\ndocker run --name s6-demo -d -p 8000:80 s6-demo use docker stop to gracefully shut down your container. s6-overlay will handle the shutdown by invoking any configured finish scripts to perform cleanup or adjust exit codes.\ns6 programs below is an overview of several key s6 programs that come with installing s6-overlay.\nexeclineb execlineb is a minimalistic command interpreter that is part of the s6 suite, designed specifically for process supervision and container management. unlike traditional shells (such as bash or sh), execlineb is not a full-featured scripting language but rather a purpose-built tool optimized for:\ndeterministic process management: it emphasizes predictable execution and precise control over processes. low overhead: it’s lightweight, making it ideal for container environments where efficiency is key. reliability: with minimal dependencies and a straightforward design, execlineb helps prevent unexpected behavior in critical system scripts. #!/command/execlineb -p # using the -p flag preserves the environment variables s6-setuidgid www-data exec /usr/local/sbin/php-fpm --nodaemonize read more about it here: https://skarnet.org/software/execline/\nwith-contenv the with-contenv command is designed to \u0026ldquo;inject\u0026rdquo; the container’s environment variables into the execution context. in containerized environments, it ensures that any environment variables set at the container level are available to the script.\nopposed to when you do not set it the program you execute will not kow about environment variables of the container.\n#!/command/with-contenv sh env s6-setuidgid always drop root privileges before running your service. in execlineb, you can do this as follows:\n#!/command/execlineb -p s6-setuidgid www-data myservice or in a shell script:\n#!/bin/sh exec s6-setuidgid www-data myservice read more about it here: https://skarnet.org/software/s6/s6-setuidgid.html\ncustomizing behavior with environment variables s6-overlay offers a range of environment variables for fine-tuning its behavior, such as:\ns6_behaviour_if_stage2_fails s6_cmd_wait_for_services_maxtime s6_kill_gracetime s6_logging \u0026hellip;and many more. these allow you to adapt the container’s behavior to your specific needs.\nautomating tasks with recipes after establishing your basic setup, you might want to extend functionality with additional recipes:\ndatabase migrations define an oneshot service to run your migration scripts:\n/etc/s6-overlay/s6-rc.d ├── init-migrations │ ├── type # contains \u0026#34;oneshot\u0026#34; │ └── up └── scripts └── init-migrations and in the migration script:\n#!/command/with-contenv sh s6-setuidgid www-data php /var/www/html/bin/console doctrine:migrations:migrate --no-interaction php /var/www/html/bin/console doctrine:migrations:status scheduled cron jobs symfonys scheduler component replaces cron jobs so you can run scheduled jobs in your application. for this to work you need to have a process running all the time that runs in a loop and asks every time if there is a message scheduled to be triggered. read the complete documentation here: https://symfony.com/doc/current/scheduler.html\ncreate a longrun service to handle cron-like tasks, such as consuming message queues like this:\n/etc/s6-overlay/s6-rc.d ├── svc-messenger-scheduler │ ├── dependencies.d │ │ └── svc-php-fpm │ ├── type │ └── run └── user/contents.d/svc-messenger-scheduler in /etc/s6-overlay/s6-rc.d/svc-messenger-scheduler/run\n#!/command/with-contenv sh s6-setuidgid www-data php /var/www/html/bin/console messenger:consume scheduler_default \\ --time-limit=300 --limit=10 --env=`printcontenv app_env` --quiet async message handling same as the scheduler there is a symfony messages component that will be connected to a transport and query for new messages. for that we need a worker service running that will consume messages and dispatch them to message handlers. read the complete documentation here: https://symfony.com/doc/current/messenger.html\n/etc/s6-overlay/s6-rc.d ├── svc-messenger-async │ ├── dependencies.d │ │ └── svc-php-fpm │ ├── type │ └── run └── user/contents.d/svc-messenger-async in /etc/s6-overlay/s6-rc.d/svc-messenger-async/run\n#!/command/with-contenv sh s6-setuidgid www-data php /var/www/html/bin/console messenger:consume messenger:consume async \\ --time-limit=300 --limit=1000 --env=\u0026#34;$(printcontenv app_env)\u0026#34; --quiet feature flags with s6_stage2_hook sometimes you want to deploy a container an turn services on or off depending on the function the container will have.\nthose are the scenarios i came up with:\nmigrations: you want to run migrations but do not want to have to ssh onto the machine where the container is running, exec into it and then execute the script that will execute the migrations. you want to run them when the container starts, but you do not want to execute them always so you want a way how to disable them. worker: you want to run a container not accessible by http, but run worker scripts like for symfony/scheduler or symfony/messenger. those would not need php-fpm or nginx running. and the workers would not need to run in the same container where the http accessible application is running. but in development it is a different story, there you want to run both. everything is easy with this feature flag script set with the s6_stage2_hook. it enables or disables specific services at startup, tailoring your container behavior to your deployment environment.\nin the dockerfile:\n# set path to feature-toggle script env s6_stage2_hook=/etc/s6-overlay/s6-hook/feature-toggle the feature-toggle script\n#!/command/with-contenv sh init_migrations=\u0026#34;${feature_init_migrations:-false}\u0026#34; svc_messenger_scheduler=\u0026#34;${feature_run_queue_scheduler:-false}\u0026#34; svc_messenger_async=\u0026#34;${feature_run_queue_async:-false}\u0026#34; svc_nginx=\u0026#34;${feature_run_nginx:-true}\u0026#34; for feature in \u0026#34;init_migrations svc_messenger_scheduler svc_nginx\u0026#34;; do is_enabled=$(eval echo \\${$feature:-false}) feature_file=$(echo \u0026#34;$feature\u0026#34; | tr \u0026#39;[:upper:]\u0026#39; \u0026#39;[:lower:]\u0026#39; | tr \u0026#39;_\u0026#39; \u0026#39;-\u0026#39;) if [ $is_enabled = false ]; then echo \u0026#34;feature-toggle: info: $feature is disabled. deleting service: $feature_file\u0026#34; rm -f \u0026#34;/etc/s6-overlay/s6-rc.d/user/contents.d/$feature_file\u0026#34; fi done exit 0 in the docker-compose.yaml\nservices: app: # ... environment: app_env: \u0026#34;dev\u0026#34; feature_run_nginx: \u0026#34;true\u0026#34; feature_run_queue_scheduler: \u0026#34;true\u0026#34; feature_run_queue_async: \u0026#34;true\u0026#34; feature_init_migrations: \u0026#34;false\u0026#34; s6-overlay base image i created a base image for s6-overlay which will be build whenever s6-overlay pushes a new version.\nhttps://github.com/dazz/s6-overlay-base\nin your dockerfile\nfrom php:8.3-alpine3.21 copy --from=hakindazz/s6-overlay-base:3.2.0.2 /s6/root / # install your app here entrypoint [\u0026#34;/init\u0026#34;] the interesting part imho is the github workflow that checks if an image for the current version exists and if not it will be built. see here https://github.com/dazz/s6-overlay-base/blob/main/.github/workflows/docker-image-push.yml you can adapt it und use it in your org if you need the be able to build your own base images .\n\u0026#xf040; this is a note read the article that i wrote about it here: build your own s6-overlay base image s6-cli i developed a small cli in golang to ease creating, validating and documenting services that s6 supervises.\nthe repo: https://github.com/dazz/s6-cli the docker image: https://hub.docker.com/repository/docker/hakindazz/s6-cli \u0026#xf040; this is a note read the article that i wrote about it here: manage s6-overlay setup with s6-cli conclusion using s6-overlay in your dockerized php / symfony setup offers a robust, production-ready solution to manage multiple processes within a single container.\nwith proper process supervision, controlled startup/shutdown sequences, and versatile configuration options, s6-overlay fills the gap left by traditional docker practices. whether you’re running php-fpm, nginx, or any combination of services, this approach leads to a more stable and maintainable environment.\nby following the in-depth examples and best practices outlined above, php developers can confidently migrate their dockerized applications to the cloud, knowing that every process is well-managed and health-checked.\nreferences https://github.com/just-containers/s6-overlay https://skarnet.org/software/s6/overview.html https://serversideup.net/open-source/docker-php/docs/guide/using-s6-overlay https://www.tonysm.com/multiprocess-containers-with-s6-overlay/ https://github.com/dazz/s6-overlay-base https://github.com/dazz/s6-nginx-php-fpm how to use \u0026ndash;init parameter in docker run - stack overflow run multiple processes in a container - docker documentation how to use \u0026ndash;init parameter in docker run command - baeldung what is docker init \u0026amp; when to use it - best practices - spacelift docker run \u0026ndash;init: to the rescue of zombie processes - paolo mainardi container lifecycle | improve it with pid 1 in docker | padok ","title":"Mastering Multi-Process Containers: Running PHP Applications with s6-overlay"},{"date":"2024-12-06","image":"https://blog.dazzlog.de/posts/2024-12-06_s6-cli//s6-cli.webp","imageAlt":"","link":"https://blog.dazzlog.de/posts/2024-12-06_s6-cli/","summary":"\u003cp\u003eI developed a small cli in golang to ease creating, validating and documenting services that s6 supervises.\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eThe repo: \u003ca href=\"https://github.com/dazz/s6-cli\"\u003ehttps://github.com/dazz/s6-cli\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003eThe docker image: \u003ca href=\"https://hub.docker.com/repository/docker/hakindazz/s6-cli\"\u003ehttps://hub.docker.com/repository/docker/hakindazz/s6-cli\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"usage\"\u003eUsage\u003c/h2\u003e\n\u003cp\u003eYou do not need to install anything, just execute the binary via docker\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run -it --rm hakindazz/s6-cli \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003ehelp\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003eCOMMANDS:\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   create, c   create a service\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   remove, rm  remove a service\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   lint, l     lint directories and files\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   mermaid, m  document s6 service dependencies in mermaid syntax\n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e   help, h     Shows a list of commands or \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003ehelp\u003c/span\u003e \u003cspan style=\"color:#ff79c6\"\u003efor\u003c/span\u003e one \u003cspan style=\"color:#8be9fd;font-style:italic\"\u003ecommand\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"create-a-service-with-s6-cli\"\u003eCreate a service with s6-cli\u003c/h2\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-shell\" data-lang=\"shell\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003edocker run -it --rm -v ./:/etc/s6-overlay hakindazz/s6-cli create oneshot init-dependencies\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eHere is the file / directory structure it creates:\u003c/p\u003e","tags":["ci","devops","docker","s6-overlay"],"text":"i developed a small cli in golang to ease creating, validating and documenting services that s6 supervises.\nthe repo: https://github.com/dazz/s6-cli the docker image: https://hub.docker.com/repository/docker/hakindazz/s6-cli usage you do not need to install anything, just execute the binary via docker\ndocker run -it --rm hakindazz/s6-cli help commands: create, c create a service remove, rm remove a service lint, l lint directories and files mermaid, m document s6 service dependencies in mermaid syntax help, h shows a list of commands or help for one command create a service with s6-cli docker run -it --rm -v ./:/etc/s6-overlay hakindazz/s6-cli create oneshot init-dependencies here is the file / directory structure it creates:\n/etc/s6-overlay/s6-rc.d ├── init-dependencies │ ├── dependencies.d │ │ ├── base │ │ └── svc-php-fpm │ ├── type │ └── up └── scripts └── init-dependencies the base dependency is added by s6-cli adding base as default dependency tells s6-rc to only start a service when all the base services are ready, and it prevents race conditions use s6-cli in your ci docker run -it --rm -v .:/etc/s6-overlay hakindazz/s6-cli lint it will tell you when it does not find any issue\ns6-cli: lint found no issues or list the findings so you can fix them before you deploy.\ns6-cli: lint found issues with services in /etc/s6-overlay/s6-rc.d - svc-lint-me - type file for \u0026#34;svc-lint-me\u0026#34; does not end with a newline - invalid type in svc-lint-me/type file specified document your setup with mermaid you can use the mermaid command to output the dependency graph between the services:\ndocker run -it --rm -v .:/etc/s6-overlay hakindazz/s6-cli mermaid ```mermaid graph td; user --\u0026gt; init-dependencies user --\u0026gt; init-migrations user --\u0026gt; svc-nginx init-migrations --\u0026gt; init-directories svc-php-fpm --\u0026gt; init-directories svc-nginx --\u0026gt; init-nginx svc-nginx --\u0026gt; svc-php-fpm which will result in this mermaid\ngraph td; user --\u003e init-dependencies user --\u003e init-migrations user --\u003e svc-nginx init-migrations --\u003e init-directories svc-php-fpm --\u003e init-directories svc-nginx --\u003e init-nginx svc-nginx --\u003e svc-php-fpm ","title":"Manage s6-overlay setup with s6-cli"},{"date":"2024-08-19","image":"https://blog.dazzlog.de/posts/2024-08-19_base-image-with-s6-overlay//s6-overlay.webp","imageAlt":"","link":"https://blog.dazzlog.de/posts/2024-08-19_base-image-with-s6-overlay/","summary":"\u003cp\u003eS6-overlay is a container-focused process manager that offers end-to-end management of the container\u0026rsquo;s lifecycle, from initialization to graceful shutdown.\u003c/p\u003e\n\u003cp\u003eTo make use of s6-overlay we need to add the binaries to our container by adding, extracting and then moving them to the directory where they are expected.\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-Dockerfile\" data-lang=\"Dockerfile\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eADD\u003c/span\u003e https://github.com/just-containers/s6-overlay/releases/download/3.2.0.0/s6-overlay-noarch.tar.xz /tmp  \n\u003c/span\u003e\u003c/span\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003e\u003cspan style=\"color:#ff79c6\"\u003eADD\u003c/span\u003e https://github.com/just-containers/s6-overlay/releases/download/3.2.0.0/s6-overlay-x86_64.tar.xz /tmp\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003ch2 id=\"update-dependencies\"\u003eUpdate dependencies\u003c/h2\u003e\n\u003cp\u003eWhen adding the s6-overlay sources to in a \u003ccode\u003eDockerfile\u003c/code\u003e we want to make sure that we get notified when a new version\nis available, so we can always be up-to-date with all our libraries. This can be achieved by adding a section to our \u003ca href=\"https://github.com/renovatebot/renovate\"\u003eRenovate\u003c/a\u003e or \u003ca href=\"https://github.com/dependabot\"\u003eDependabot\u003c/a\u003e config, a rule to match\u003c/p\u003e","tags":["devops","docker","s6-overlay"],"text":"s6-overlay is a container-focused process manager that offers end-to-end management of the container\u0026rsquo;s lifecycle, from initialization to graceful shutdown.\nto make use of s6-overlay we need to add the binaries to our container by adding, extracting and then moving them to the directory where they are expected.\nadd https://github.com/just-containers/s6-overlay/releases/download/3.2.0.0/s6-overlay-noarch.tar.xz /tmp add https://github.com/just-containers/s6-overlay/releases/download/3.2.0.0/s6-overlay-x86_64.tar.xz /tmp update dependencies when adding the s6-overlay sources to in a dockerfile we want to make sure that we get notified when a new version is available, so we can always be up-to-date with all our libraries. this can be achieved by adding a section to our renovate or dependabot config, a rule to match\nversion checker know about docker from if we leverage the from of docker to include our sources we would not need to add anything. i already use the way of loading sources via images in several places:\ninclude composer from composer:2.7.7 as composer copy --from=composer /usr/bin/composer /usr/bin/composer include extension-installer from mlocati/php-extension-installer:2.2.16 as php-extension-installer copy --from=php-extension-installer /usr/bin/install-php-extensions /usr/local/bin/ run install-php-extensions \\ xdebug \\ zip \\ ; but there is no base image for s6-overlay from justcontainers/s6-overlay . there are some other vendors, but they are opinionated and do more things that are helpful to their case.\nbuild your own s6-overlay base image from alpine:3 as s6 arg targetarch arg targetvariant arg s6_release run apk add --no-cache curl jq \\ \u0026amp;\u0026amp; if [ -z ${s6_release} ]; then \\ s6_release=$(curl -s https://api.github.com/repos/just-containers/s6-overlay/releases/latest | jq -r \u0026#39;.tag_name\u0026#39; | cut -c2-); \\ fi \\ \u0026amp;\u0026amp; s6_platform=$(case \u0026#34;${targetarch}/${targetvariant}\u0026#34; in \\ \u0026#34;arm/v7\u0026#34;) echo \u0026#34;armhf\u0026#34;;; \\ \u0026#34;arm64/\u0026#34;) echo \u0026#34;aarch64\u0026#34;;; \\ *) echo \u0026#34;x86_64\u0026#34;;; \\ esac) \\ \u0026amp;\u0026amp; echo \u0026#34;using s6 release ${s6_release} platform ${s6_platform}\u0026#34; \\ \u0026amp;\u0026amp; curl -ssl \u0026#34;https://github.com/just-containers/s6-overlay/releases/download/v${s6_release}/s6-overlay-noarch.tar.xz\u0026#34; -o \u0026#34;/tmp/s6-overlay-noarch.tar.xz\u0026#34; \\ \u0026amp;\u0026amp; curl -ssl \u0026#34;https://github.com/just-containers/s6-overlay/releases/download/v${s6_release}/s6-overlay-${s6_platform}.tar.xz\u0026#34; -o \u0026#34;/tmp/s6-overlay-${s6_platform}.tar.xz\u0026#34; \\ \u0026amp;\u0026amp; curl -ssl \u0026#34;https://github.com/just-containers/s6-overlay/releases/download/v${s6_release}/s6-overlay-noarch.tar.xz.sha256\u0026#34; -o \u0026#34;/tmp/s6-overlay-noarch.tar.xz.sha256\u0026#34; \\ \u0026amp;\u0026amp; curl -ssl \u0026#34;https://github.com/just-containers/s6-overlay/releases/download/v${s6_release}/s6-overlay-${s6_platform}.tar.xz.sha256\u0026#34; -o \u0026#34;/tmp/s6-overlay-${s6_platform}.tar.xz.sha256\u0026#34; \\ \u0026amp;\u0026amp; cd /tmp \\ \u0026amp;\u0026amp; sha256sum -c s6-overlay-noarch.tar.xz.sha256 \\ \u0026amp;\u0026amp; sha256sum -c s6-overlay-${s6_platform}.tar.xz.sha256 \\ \u0026amp;\u0026amp; mkdir -p /s6/root \\ \u0026amp;\u0026amp; tar -c /s6/root -jxpf /tmp/s6-overlay-noarch.tar.xz \\ \u0026amp;\u0026amp; tar -c /s6/root -jxpf /tmp/s6-overlay-${s6_platform}.tar.xz from scratch copy --from=s6 /s6/root /s6/root it\ndownloads for the specified architecture asserts the checksum! uses a fresh layer to copy everything this can be built with:\ndocker build --no-cache --build-arg s6_release=3.2.0.0 -t hakindazz/s6-overlay-base:3.2.0.0 . or you can pull the image to check it out:\ndocker pull hakindazz/s6-overlay-base:3.2.0.0 include via docker from the best part: you can now include the versioned sources via docker --from:\nfrom hakindazz/s6-overlay-base as s6-overlay from alpine3 copy --from=s6-overlay /s6/root / entrypoint [\u0026#34;/init\u0026#34;] happy image building!!! some sources: https://github.com/just-containers/s6-overlay https://github.com/dazz/s6-overlay-base https://hub.docker.com/r/hakindazz/s6-overlay-base https://github.com/dependabot https://docs.renovatebot.com/modules/datasource/github-releases/ ","title":"Build your own s6-overlay base image"},{"date":"2023-01-02","image":"https://blog.dazzlog.de/posts/2023-01-02_environment-variables-in-a-dockerized-symfony//amy-humphries-2M_sDJ_agvs-unsplash.jpg","imageAlt":"","link":"https://blog.dazzlog.de/posts/2023-01-02_environment-variables-in-a-dockerized-symfony/","summary":"\u003cp\u003eI have developed a \u003cstrong\u003eSymfony Web-Application\u003c/strong\u003e, and it runs locally in a dockerized environment with docker-compose. This app is going to be deployed to production as a docker container.\nIn production the handling of environment variables and how they are passed to the container during development is different.\u003c/p\u003e\n\u003ch2 id=\"12-factor-app\"\u003e12 Factor App\u003c/h2\u003e\n\u003cp\u003eA few points from the \u003ca href=\"https://12factor.net\"\u003e12factor methodology\u003c/a\u003e:\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://12factor.net/config\"\u003eIII. Config\u003c/a\u003e: Store config in the environment since env vars are easy to change between deploys without changing any code\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://12factor.net/dev-prod-parity\"\u003eX. Dev/prod parity\u003c/a\u003e: Keep development, staging, and production as similar as possible\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003eI was searching for options how to handle the differences how environment variables are passed and I found there are at least\u003c/p\u003e","tags":["cd","ci","docker","docker-compose","dotenv","env_file","symfony"],"text":"i have developed a symfony web-application, and it runs locally in a dockerized environment with docker-compose. this app is going to be deployed to production as a docker container. in production the handling of environment variables and how they are passed to the container during development is different.\n12 factor app a few points from the 12factor methodology:\niii. config: store config in the environment since env vars are easy to change between deploys without changing any code x. dev/prod parity: keep development, staging, and production as similar as possible i was searching for options how to handle the differences how environment variables are passed and i found there are at least\n7 ways to pass environment variables to a container env in dockerfile dockerfile args passed at build time to env env passing in docker run as option env_file in docker run as option environment variables in docker-compose.yml env_file in docker compose for each service .env in docker compose substitutes variables in docker-compose.yml and there is even more. if variables are passed to a container there is an order of precedence as follows:\n\u0026#xf400; order of precedence passed from the command line docker compose run --env \u0026lt;key[=[val]]\u0026gt;. passed from/set in compose.yaml service’s configuration, from the environment key. passed from/set in compose.yaml service’s configuration, from the env_file key. passed from/set in container image in the env directive. from https://docs.docker.com/compose/envvars-precedence/\nhow to deal with environment variables in a dockerized symfony the goal all services regardless of which technology they use, should have one streamlined way of how the environment variables should be passed to the application.\nthe big picture we use multiple services which all need to work together services run in docker container we deploy and run services in different compositions for each environment each service has their own sensitive data each service might be a different technology or has a different tech stack steps towards the goal the infrastructure config should be kept in env files but not in the same directory as the application each service gets its own env file to be completely independent of each other, and it gets explicitly set during development each service gets the env variables passed via env file (env_file in docker-compose) every project that has a docker-compose.yml moves the application into an app directory to separate the application from its infrastructure configuration we remove the dotenv component from symfony and define each environment variable that we expect as parameter so the app tells us instantly when a key-value pair is missing in development credentials can be added to the vcs in all other envs the credentials can be either stored and linked on the server or be read from a vault the implementation in symfony the dotenv component is default installed and enabled in the frontcontroller, so when a new app is created there is always a .env file at the project root created with it. read more in the documentation.\nit is not the same .env that docker-compose.yml expects. \u0026#xf071; symfony dotenv and docker compose use the same file name .env docker compose is also using a file named .env to replace the variables in the docker-compose.yml if it is located in the same directory. if you don\u0026rsquo;t know that and put the web apps .env file in the same place then you accidentally might overwrite variables when you think you just updated a variable for the symfony application. we have two different stacks here that both want to use the .env file and both might, but not at the same time, obviously.\nsince we want to use config variables explicitly and not by accident the symfony dotenv component is going to be removed and all config is moved inside environment variable files that are passed into the container.\nthe directory tree to ease the separation of infrastructure and code the application code moves into the ./app directory to be completely separate from the code/config that defines the infrastructure. you see there is no .env file left from symfony. all variables have now moved to the env files inside the devops/env directory.\n. ├── app │ ├── assets │ ├── bin │ ├── ci │ ├── config │ ├── migrations │ ├── node_modules │ ├── public │ │ └── index.php │ ├── src │ ├── templates │ ├── tests │ ├── var │ ├── vendor │ ├── composer.json │ ├── composer.lock │ ├── makefile │ ├── package.json │ ├── symfony.lock │ ├── webpack.config.js │ └── yarn.lock ├── devops │ ├── database │ ├── docker │ │ └── frankenphp │ │ └── dockerfile │ └── env │ ├── app.env │ └── database.env ├── contributing.md ├── docker-compose.prod.yml ├── docker-compose.yml ├── makefile └── readme.md the docker-compose.yml each service gets its own env_file where we can configure the sensitive data for each service.\nversion: \u0026#39;3.9\u0026#39; services: app: image: ghcr.io/c-base/cbag3:dev-latest build: dockerfile: ./devops/docker/frankenphp/dockerfile target: dev env_file: ./devops/env/app.env ports: - 80:80 - 443:443 volumes: - \u0026#39;./app:/app\u0026#39; database: image: postgres:alpine container_name: database env_file: ./devops/env/database.env ports: - 15432:5432 volumes: - ./devops/database:/var/lib/postgresql \u0026#xf400; tip the .env file can be used with docker compose to configure variables inside the docker-compose.yml disable dotenv in frontcontroller and console the dotenv component is disabled since all environment variables have already passed to the container.\n# app/public/index.php \u0026lt;?php use cbase\\app\\kernel; $_server[\u0026#39;app_runtime_options\u0026#39;][\u0026#39;disable_dotenv\u0026#39;] = true; require_once dirname(__dir__).\u0026#39;/vendor/autoload_runtime.php\u0026#39;; return function (array $context) { return new kernel($context[\u0026#39;app_env\u0026#39;], (bool) $context[\u0026#39;app_debug\u0026#39;]); }; # app/bin/console #!/usr/bin/env php \u0026lt;?php use cbase\\app\\kernel; use symfony\\bundle\\frameworkbundle\\console\\application; $_server[\u0026#39;app_runtime_options\u0026#39;][\u0026#39;disable_dotenv\u0026#39;] = true; require_once dirname(__dir__) . \u0026#39;/vendor/autoload_runtime.php\u0026#39;; return function (array $context) { $kernel = new kernel($context[\u0026#39;app_env\u0026#39;], (bool) $context[\u0026#39;app_debug\u0026#39;]); return new application($kernel); }; \u0026#xe756; symfony runtime see described here: configure symfony runtime using options \u0026#xf040; run app only inside container by disabling dotenv we will no longer be able to run the application outside the container (our local machine) unless we set all environment variables there as well. \u0026#xf040; keep dotenv for tests for now, we leave the dotenv for the tests since those environment variables won\u0026rsquo;t change regardless of where they are executed, and they will be executed in the dev container. we could change that by running the tests in their own container, but for now keep the .env.test. don\u0026rsquo;t forget to add the parameters in services.yml # app/config/services.yaml parameters: images.upload.directory: \u0026#39;%env(resolve:images_upload_directory)%\u0026#39; services: _defaults: autowire: true autoconfigure: true bind: string $imagesuploaddirectory: \u0026#39;%images.upload.directory%\u0026#39; since every environment has its own env_file there is the danger of forgetting to add an environment variable to the other environments.\n\u0026#xf400; tip in order to fail early we load the environment variable at the start of the application, by binding it. if we do not bind parameters to variables but just bin them to a service we might miss that we forgot to set an environment variable in the env file since the service might not be loaded in every request. run docker container in production with env-file cat devops/env/app.env # this is a comment images_upload_directory=\u0026#34;%kernel.project_dir%/var/uploads\u0026#34; docker run --env-file devops/env/app.env app env | grep -e \u0026#39;images\u0026#39; images_upload_directory=\u0026#34;%kernel.project_dir%/var/uploads\u0026#34; read more about it in the docker documentation.\nmigration path there is a migration path for projects that use already many config yaml files and want to migrate to environment variables.\n# config/my-app.yaml parameters: images.upload.directory: \u0026#39;%kernel.project_dir%/var/uploads\u0026#39; # config/services.yaml parameters: env(images_upload_directory): \u0026#39;%images.upload.directory%\u0026#39; services: _defaults: bind: string $imagesuploaddirectory: \u0026#39;%env(resolve:images_upload_directory)%\u0026#39; the configuration processor looks up if there is an environment variable images_upload_directory if that is the case, it will be taken, otherwise if it is not found '%images.upload.directory%' will be set to the environment variable. the '%env(resolve:images_upload_directory)%' is bound to a variable $imagesuploaddirectory read more about configuration processors in the symfony documentation about \u0026ldquo;environment variable processors\u0026rdquo;.\nthis would result in the following migration path:\nmake it possible to set variables via environment variables make sure all environments set the corresponding variables remove many quirky unnecessary config files win conclusion we removed the dotenv from symfony and will miss out on all the functionality that came with it, but chose using the env_file as it can be used for running a container, and it can be configured in the docker-compose.yml. the environment configs can be dumped from secret vaults regardless of the tech-stack that the cloud has to offer or kept in a shared directory that won\u0026rsquo;t change between deployments. there will be one explicit way of how each service will get configuration regardless of their environment or tech stack. also, we learned that there is a simple way in symfony to migrate to environment variables.\nhappy continuously deploying everyone more sources environment variables in container vs. docker compose file ","title":"Environment variables in a dockerized Symfony"},{"date":"2022-12-28","image":"https://blog.dazzlog.de/posts/2022-12-28_build-and-push-docker-images-to-ghcr-with-github-actions//dockergithub.png","imageAlt":"","link":"https://blog.dazzlog.de/posts/2022-12-28_build-and-push-docker-images-to-ghcr-with-github-actions/","summary":"\u003cp\u003eWhen you host your project code on GitHub and want to release it as a docker image for deployment or just publish it, the way to go are GitHub actions. Actions are basically hooks that can start CI/DC workflows on repository events.\u003c/p\u003e\n\u003cp\u003eGitHub actions can be used to build and push images to GitHub’s Container Registry which are reachable under \u003ca href=\"https://ghcr.io\"\u003ehttps://ghcr.io\u003c/a\u003e which is part of the package registry. The package registry is not only for docker images, it can also host quite a few other kinds of packages. In this case we’ll focus on docker images.\u003c/p\u003e","tags":["cd","docker","github","github-actions","github-packages"],"text":"when you host your project code on github and want to release it as a docker image for deployment or just publish it, the way to go are github actions. actions are basically hooks that can start ci/dc workflows on repository events.\ngithub actions can be used to build and push images to github’s container registry which are reachable under https://ghcr.io which is part of the package registry. the package registry is not only for docker images, it can also host quite a few other kinds of packages. in this case we’ll focus on docker images.\nprerequisites: github repository basic knowledge about github actions syntax dockerfile the github workflow i created a workflow in my repository under .github/workflow/cd.md and added the following:\nname: continuous delivery on: push: branches: - \u0026#39;main\u0026#39; tags: - \u0026#39;v*.*.*\u0026#39; jobs: build: name: buid and push docker image to github container registry runs-on: ubuntu-latest permissions: packages: write contents: read steps: - name: checkout the repository uses: actions/checkout@v3 - name: docker setup buildx uses: docker/setup-buildx-action@v2.2.1 - name: docker login uses: docker/login-action@v2.1.0 with: registry: ghcr.io username: ${{ github.actor }} password: ${{ secrets.github_token }} - name: build and push docker images uses: docker/build-push-action@v3.2.0 env: registry: ghcr.io image_name: ${{ github.repository }} with: context: . file: ./dockerfile target: final push: true tags: ${{ env.registry }}/${{ env.image_name }}:latest let’s go through the important parts: permissions: actions have access to the repo while running. we should always make sure by setting the permissions, that actions have the minimum access they require. see here: permissions for the github_token\nstep 1–2: checkout the code and setup docker\nstep 3: login to github container registry: this is where the interesting part starts. github.actor is the user that triggers the workflow. for password use secrets.github_token which is a temporary token which is automatically generated for this workflow. see here: publishing images to github-packages.\nstep 4: build and push docker images: if the registry that you want to push to belongs to an organization then you will need to add permissions to create packages. if it lives under your own handle you don’t need to configure anything more since you are the owner already and the secrets.github_token has all the permissions granted.\nthe action will consume the dockerfile and build the image up to the target build step that you can define. in docker the repository where the image will be hosted is also part of the tag. setting the image name to the repository name will create an image with the following tag: ghcr.io/owner/image_name:latest\nread more here: pushing container images\nhappy shipping \\o/ ","title":"Build and push docker images to ghcr.io with GitHub Actions"},{"date":"2022-12-27","image":"https://blog.dazzlog.de/posts/2022-12-27_goodbye-jekyl-hello-hugo//hugo.png","imageAlt":"","link":"https://blog.dazzlog.de/posts/2022-12-27_goodbye-jekyl-hello-hugo/","summary":"\u003cp\u003eI started this blog in March 2013 when I was working for ImagineEasy when I had a few ideas to write down on how I\u0026rsquo;d work with Doctrine Repositories. I still like the idea, but I\u0026rsquo;d probably do it a bit different today. The blog and also how I\u0026rsquo;d work with doctrine.\u003c/p\u003e\n\u003cp\u003eAt the time Jekyll was the way to handle a static file blog. Since then, again, a few things have changed. GitHub is now owned by Microsoft and there are GitHub Actions.\u003c/p\u003e","tags":["blog","cd","git","github","github-actions","hugo","jekyll"],"text":"i started this blog in march 2013 when i was working for imagineeasy when i had a few ideas to write down on how i\u0026rsquo;d work with doctrine repositories. i still like the idea, but i\u0026rsquo;d probably do it a bit different today. the blog and also how i\u0026rsquo;d work with doctrine.\nat the time jekyll was the way to handle a static file blog. since then, again, a few things have changed. github is now owned by microsoft and there are github actions.\njekyll still exists, but all in all i think it was a modern choice at a different time. so what\u0026rsquo;s next then? i must admit i did not look far, it was more a zeitgeist thing that just ran my way.\nsomeone mentioned hugo as a blog that you can easily publish to from obsidian, i\u0026rsquo;m not planning on using that particular feature, but i looked hugo up, and it seems that it is exactly what i was to lazy to look for. a static site generator that uses markdown and can be build by github actions.\nthere is already a huge list of possible themes and many look promising from the thumbnail, but feature wise there is a huge difference. what to look out for:\nhow far is the template deviating from the default, in case you ever want to change the template. which features do you want/need and are they already included, is it complicated to add them later? are there existing installations that are actively used, so they can be used as a reference is there a reference/documentation on how to install it on the host how do you publish new content, are there examples/documentation? what is the version of the technologies in the deploy-chain, old? for every point there is also the question of how complicated each step is.\nas a reference, this is what i ended up doing:\nlocal setup install hugo sudo apt-get install hugo create a new site see the official quick start for more infos.\nhugo new site dazz.github.io cd dazz.github.io git init add a theme look at all the blog themes hugo already has listed.\ni chose hugo-ficurinia as it has the following enabled: tags, categories, fonts i like, simply deploys and looks as promised.\nadd the theme as submodule:\ngit submodule add https://gitlab.com/gabmus/hugo-ficurinia themes/hugo-ficurinia and run a local server to test everything\nhugo server -t hugo-ficurinia add a blog post there is probably nothing much to see, so let\u0026rsquo;s add a new post as draft.\nhugo new posts/hello-world/index.md make sure to read about how to organize the content in directories.\nrun the server and run the server again and also include the draft post\nhugo server -t hugo-ficurinia --builddrafts when you run just hugo -t hugo-ficurinia the site will be build and dumped to public/. that is what we will later do to deploy the site.\ntime to commit all the changed files and add the remote to push everything\ngit remote add origin git@github.com:dazz/dazz.github.io.git git push origin main there will nothing happen yet as we still need to add the github workflow\ndeploy to github pages via github actions mkdir -p .github/workflows touch .github/workflows/pages-deploy.yml # file: .github/workflows/pages-deploy.yml name: \u0026#34;build and deploy gh-pages\u0026#34; on: push: branches: - main paths-ignore: - .gitignore - readme.md - license # allows you to run this workflow manually from the actions tab workflow_dispatch: permissions: contents: write # needed to push to the gh-pages branch pages: write id-token: write # allow one concurrent deployment concurrency: group: \u0026#34;pages\u0026#34; cancel-in-progress: true jobs: build: runs-on: ubuntu-latest steps: # step 1 - checks-out your repository under $github_workspace - name: checkout uses: actions/checkout@v3 with: fetch-depth: 0 submodules: true # step 2 - sets up the latest version of hugo - name: hugo setup uses: peaceiris/actions-hugo@v2.6.0 with: extended: true hugo-version: \u0026#39;latest\u0026#39; # step 3 - adds a cache - uses: actions/cache@v2 with: path: /tmp/hugo_cache key: ${{ runner.os }}-hugomod-${{ hashfiles(\u0026#39;**/go.sum\u0026#39;) }} restore-keys: | ${{ runner.os }}-hugomod- # step 4 - clean and don\u0026#39;t fail - name: clean public directory run: rm -rf public/* # step 5 - builds the site using the latest version of hugo # also specifies the theme we want to use - name: build run: hugo --minify --theme=hugo-ficurinia # step 6 - push our generated site to our gh-pages branch - name: github pages action uses: peaceiris/actions-gh-pages@v3.9.0 with: github_token: ${{ secrets.github_token }} publish_dir: ./public cname: dazz.github.io read more about the options you get when using peaceiris/actions-hugo and peaceiris/actions-gh-pages.\ncustomization now the customization party can start. all the options are listed in the config.toml.\nhappy blogging \\o/ ","title":"Goodbye Jekyll, hello Hugo"},{"date":"2022-12-26","image":"","imageAlt":"","link":"https://blog.dazzlog.de/posts/2022-12-26_hello-world/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis is \u003cstrong\u003ebold\u003c/strong\u003e text, and this is \u003cem\u003eemphasized\u003c/em\u003e text.\u003c/p\u003e\n\u003cp\u003eVisit the \u003ca href=\"https://gohugo.io\"\u003eHugo\u003c/a\u003e website!\u003c/p\u003e","tags":["first"],"text":"introduction this is bold text, and this is emphasized text.\nvisit the hugo website!\n","title":"Hello, world!"},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://blog.dazzlog.de/about/","summary":"\u003cp\u003eI love programming and most parts of it is trying to figure out how things can be used to make the life easier for everyone.\u003c/p\u003e\n\u003cp\u003eSymfony is making my life easier since 2012, and since 2021 I\u0026rsquo;m the organizer of the Symfony User Group Berlin.\u003c/p\u003e\n\u003cp\u003eI\u0026rsquo;m member of c-base, the crashed space station in berlin and there I experiment with all the other carbon based\nbeings how to interact in a future compatible way.\u003c/p\u003e","tags":["berlin","dazz","symfony"],"text":"i love programming and most parts of it is trying to figure out how things can be used to make the life easier for everyone.\nsymfony is making my life easier since 2012, and since 2021 i\u0026rsquo;m the organizer of the symfony user group berlin.\ni\u0026rsquo;m member of c-base, the crashed space station in berlin and there i experiment with all the other carbon based beings how to interact in a future compatible way.\nsometimes i talk at conferences about things i discover and think they might be useful for others.\nreferences logo: bild von catalyststuff auf freepik ","title":"about"},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://blog.dazzlog.de/impressum/","summary":"\u003ch2 id=\"angaben-gemäß--5-tmg\"\u003eAngaben gemäß § 5 TMG\u003c/h2\u003e\n\u003cp\u003eAnne Seitz\u003c/p\u003e\n\u003cp\u003eTempelhofer Ufer 1\u003c/p\u003e\n\u003cp\u003e10961 Berlin\u003c/p\u003e\n\u003ch3 id=\"kontakt\"\u003eKontakt:\u003c/h3\u003e\n\u003cp\u003eE-Mail: \u003ca href=\"mailto:dazzlog@gmail.com\"\u003edazzlog@gmail.com\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"verantwortlich-für-den-inhalt-nach--55-abs-2-rstv\"\u003eVerantwortlich für den Inhalt nach § 55 Abs. 2 RStV:\u003c/h2\u003e\n\u003cp\u003eAnne Seitz\u003c/p\u003e\n\u003cp\u003eTempelhofer Ufer 1\u003c/p\u003e\n\u003cp\u003e10961 Berlin\u003c/p\u003e\n\u003ch2 id=\"haftungsausschluss\"\u003eHaftungsausschluss:\u003c/h2\u003e\n\u003ch3 id=\"haftung-für-inhalte\"\u003eHaftung für Inhalte\u003c/h3\u003e\n\u003cp\u003eDie Inhalte unserer Seiten wurden mit größter Sorgfalt erstellt. Für die Richtigkeit, Vollständigkeit und Aktualität der Inhalte können wir jedoch keine Gewähr übernehmen. Als Diensteanbieter sind wir gemäß § 7 Abs.1 TMG für eigene Inhalte auf diesen Seiten nach den allgemeinen Gesetzen verantwortlich. Nach §§ 8 bis 10 TMG sind wir als Diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde Informationen zu überwachen oder nach Umständen zu forschen, die auf eine rechtswidrige Tätigkeit hinweisen. Verpflichtungen zur Entfernung oder Sperrung der Nutzung von Informationen nach den allgemeinen Gesetzen bleiben hiervon unberührt. Eine diesbezügliche Haftung ist jedoch erst ab dem Zeitpunkt der Kenntnis einer konkreten Rechtsverletzung möglich. Bei Bekanntwerden von entsprechenden Rechtsverletzungen werden wir diese Inhalte umgehend entfernen.\u003c/p\u003e","tags":[],"text":"angaben gemäß § 5 tmg anne seitz\ntempelhofer ufer 1\n10961 berlin\nkontakt: e-mail: dazzlog@gmail.com\nverantwortlich für den inhalt nach § 55 abs. 2 rstv: anne seitz\ntempelhofer ufer 1\n10961 berlin\nhaftungsausschluss: haftung für inhalte die inhalte unserer seiten wurden mit größter sorgfalt erstellt. für die richtigkeit, vollständigkeit und aktualität der inhalte können wir jedoch keine gewähr übernehmen. als diensteanbieter sind wir gemäß § 7 abs.1 tmg für eigene inhalte auf diesen seiten nach den allgemeinen gesetzen verantwortlich. nach §§ 8 bis 10 tmg sind wir als diensteanbieter jedoch nicht verpflichtet, übermittelte oder gespeicherte fremde informationen zu überwachen oder nach umständen zu forschen, die auf eine rechtswidrige tätigkeit hinweisen. verpflichtungen zur entfernung oder sperrung der nutzung von informationen nach den allgemeinen gesetzen bleiben hiervon unberührt. eine diesbezügliche haftung ist jedoch erst ab dem zeitpunkt der kenntnis einer konkreten rechtsverletzung möglich. bei bekanntwerden von entsprechenden rechtsverletzungen werden wir diese inhalte umgehend entfernen.\nhaftung für links unser angebot enthält links zu externen webseiten dritter, auf deren inhalte wir keinen einfluss haben. deshalb können wir für diese fremden inhalte auch keine gewähr übernehmen. für die inhalte der verlinkten seiten ist stets der jeweilige anbieter oder betreiber der seiten verantwortlich. die verlinkten seiten wurden zum zeitpunkt der verlinkung auf mögliche rechtsverstöße überprüft. rechtswidrige inhalte waren zum zeitpunkt der verlinkung nicht erkennbar. eine permanente inhaltliche kontrolle der verlinkten seiten ist jedoch ohne konkrete anhaltspunkte einer rechtsverletzung nicht zumutbar. bei bekanntwerden von rechtsverletzungen werden wir derartige links umgehend entfernen.\nurheberrecht die durch die seitenbetreiber erstellten inhalte und werke auf diesen seiten unterliegen dem deutschen urheberrecht. die vervielfältigung, bearbeitung, verbreitung und jede art der verwertung außerhalb der grenzen des urheberrechtes bedürfen der schriftlichen zustimmung des jeweiligen autors bzw. erstellers. downloads und kopien dieser seite sind nur für den privaten, nicht kommerziellen gebrauch gestattet. soweit die inhalte auf dieser seite nicht vom betreiber erstellt wurden, werden die urheberrechte dritter beachtet. insbesondere werden inhalte dritter als solche gekennzeichnet. sollten sie trotzdem auf eine urheberrechtsverletzung aufmerksam werden, bitten wir um einen entsprechenden hinweis. bei bekanntwerden von rechtsverletzungen werden wir derartige inhalte umgehend entfernen.\ndatenschutz die nutzung unserer webseite ist in der regel ohne angabe personenbezogener daten möglich. soweit auf unseren seiten personenbezogene daten (beispielsweise name, anschrift oder email-adressen) erhoben werden, erfolgt dies, soweit möglich, stets auf freiwilliger basis. diese daten werden ohne ihre ausdrückliche zustimmung nicht an dritte weitergegeben. wir weisen darauf hin, dass die datenübertragung im internet (z.b. bei der kommunikation per e-mail) sicherheitslücken aufweisen kann. ein lückenloser schutz der daten vor dem zugriff durch dritte ist nicht möglich. der nutzung von im rahmen der impressumspflicht veröffentlichten kontaktdaten durch dritte zur übersendung von nicht ausdrücklich angeforderter werbung und informationsmaterialien wird hiermit ausdrücklich widersprochen. die betreiber der seiten behalten sich ausdrücklich rechtliche schritte im falle der unverlangten zusendung von werbeinformationen, etwa durch spam-mails, vor.\ngoogle analytics diese website benutzt google analytics, einen webanalysedienst der google inc. (\u0026lsquo;\u0026lsquo;google\u0026rsquo;\u0026rsquo;). google analytics verwendet sog. \u0026lsquo;\u0026lsquo;cookies\u0026rsquo;\u0026rsquo;, textdateien, die auf ihrem computer gespeichert werden und die eine analyse der benutzung der website durch sie ermöglicht. die durch den cookie erzeugten informationen über ihre benutzung dieser website (einschließlich ihrer ip-adresse) wird an einen server von google in den usa übertragen und dort gespeichert. google wird diese informationen benutzen, um ihre nutzung der website auszuwerten, um reports über die websiteaktivitäten für die websitebetreiber zusammenzustellen und um weitere mit der websitenutzung und der internetnutzung verbundene dienstleistungen zu erbringen. auch wird google diese informationen gegebenenfalls an dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit dritte diese daten im auftrag von google verarbeiten. google wird in keinem fall ihre ip-adresse mit anderen daten der google in verbindung bringen. sie können die installation der cookies durch eine entsprechende einstellung ihrer browser software verhindern; wir weisen sie jedoch darauf hin, dass sie in diesem fall gegebenenfalls nicht sämtliche funktionen dieser website voll umfänglich nutzen können. durch die nutzung dieser website erklären sie sich mit der bearbeitung der über sie erhobenen daten durch google in der zuvor beschriebenen art und weise und zu dem zuvor benannten zweck einverstanden.\ngoogle adsense diese website benutzt google adsense, einen webanzeigendienst der google inc., usa (\u0026lsquo;\u0026lsquo;google\u0026rsquo;\u0026rsquo;). google adsense verwendet sog. \u0026lsquo;\u0026lsquo;cookies\u0026rsquo;\u0026rsquo; (textdateien), die auf ihrem computer gespeichert werden und die eine analyse der benutzung der website durch sie ermöglicht. google adsense verwendet auch sog. \u0026lsquo;\u0026lsquo;web beacons\u0026rsquo;\u0026rsquo; (kleine unsichtbare grafiken) zur sammlung von informationen. durch die verwendung des web beacons können einfache aktionen wie der besucherverkehr auf der webseite aufgezeichnet und gesammelt werden. die durch den cookie und/oder web beacon erzeugten informationen über ihre benutzung dieser website (einschließlich ihrer ip-adresse) werden an einen server von google in den usa übertragen und dort gespeichert. google wird diese informationen benutzen, um ihre nutzung der website im hinblick auf die anzeigen auszuwerten, um reports über die websiteaktivitäten und anzeigen für die websitebetreiber zusammenzustellen und um weitere mit der websitenutzung und der internetnutzung verbundene dienstleistungen zu erbringen. auch wird google diese informationen gegebenenfalls an dritte übertragen, sofern dies gesetzlich vorgeschrieben oder soweit dritte diese daten im auftrag von google verarbeiten. google wird in keinem fall ihre ip-adresse mit anderen daten der google in verbindung bringen. das speichern von cookies auf ihrer festplatte und die anzeige von web beacons können sie verhindern, indem sie in ihren browser-einstellungen \u0026lsquo;\u0026lsquo;keine cookies akzeptieren\u0026rsquo;\u0026rsquo; wählen (im ms internet-explorer unter \u0026lsquo;\u0026lsquo;extras \u0026gt; internetoptionen \u0026gt; datenschutz \u0026gt; einstellung\u0026rsquo;\u0026rsquo;; im firefox unter \u0026lsquo;\u0026lsquo;extras \u0026gt; einstellungen \u0026gt; datenschutz \u0026gt; cookies\u0026rsquo;\u0026rsquo;); wir weisen sie jedoch darauf hin, dass sie in diesem fall gegebenenfalls nicht sämtliche funktionen dieser website voll umfänglich nutzen können. durch die nutzung dieser website erklären sie sich mit der bearbeitung der über sie erhobenen daten durch google in der zuvor beschriebenen art und weise und zu dem zuvor benannten zweck einverstanden.\nwebsite impressum erstellt durch impressum-generator.de von der kanzlei hasselbach\n","title":"impressum"},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://blog.dazzlog.de/talks/","summary":"\u003ch2 id=\"2025\"\u003e2025\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.meetup.com/sfugcgn\"\u003eSymfony UG Cologne\u003c/a\u003e – \u003ca href=\"https://www.meetup.com/de-DE/sfugcgn/events/308227980/\"\u003eRunning Symfony in a Multi-Process Container\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.phpday.it/\"\u003ephpday Verona 2025\u003c/a\u003e – \u003ca href=\"https://www.phpday.it/talks_speakers/#Running-PHP-Applications-in-a-Multi-Process-Container\"\u003eRunning PHP-Applications in a Multi-Process Container \u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://live.symfony.com/2025-berlin/\"\u003eSymfonyLive Berlin 2025\u003c/a\u003e – \u003ca href=\"https://live.symfony.com/2025-berlin/schedule/running-symfony-in-a-multi-process-container\"\u003eRunning Symfony in a Multi-Process Container\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2024\"\u003e2024\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://live.symfony.com/2024-vienna-con/\"\u003eSymfonyCon Vienna 2024\u003c/a\u003e – \u003ca href=\"https://live.symfony.com/2024-vienna-con/schedule/running-symfony-in-a-multi-process-container\"\u003eRunning Symfony in a Multi-Process\nContainer\u003c/a\u003e | \u003ca href=\"https://speakerdeck.com/dazz/running-symfony-in-multi-process-containers\"\u003eSlides\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://phpdd.org/\"\u003e#PHPDD24\u003c/a\u003e – \u003ca href=\"https://phpdd.org/#speaker-7\"\u003eRunning PHP-Applications in a Multi-Process Container\u003c/a\u003e | \u003ca href=\"https://www.youtube.com/watch?v=fvC13yac2N8\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.meetup.com/de-DE/sfugberlin\"\u003eSymfony User Group Berlin\u003c/a\u003e - \u003ca href=\"https://www.meetup.com/de-DE/sfugberlin/events/303072411\"\u003es6-overlay: Symfony in a Multi-Process container\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2023\"\u003e2023\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://live.symfony.com/2023-brussels-con/\"\u003eSymfonyCon Brussels 2023\u003c/a\u003e – From Chaos to Control: Exception Handling in Symfony\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://live.symfony.com/2023-berlin/\"\u003eSymfonyLive Berlin 2023\u003c/a\u003e – \u003ca href=\"https://speakerdeck.com/dazz/von-chaos-zu-kontrolle-exception-handling-in-symfony\"\u003eVon Chaos zu Kontrolle: Exception Handling in\nSymfony\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://www.meetup.com/de-DE/sfugberlin\"\u003eSymfony User Group Berlin\u003c/a\u003e - \u003ca href=\"https://www.meetup.com/de-DE/sfugberlin/events/296791213\"\u003eFrom Chaos to Control: Exception Handling in Symfony\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2018\"\u003e2018\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://live.symfony.com/2018-berlin/\"\u003eSymfonyLive Berlin 2018\u003c/a\u003e – Wie man Dinge gut benennt | \u003ca href=\"https://speakerdeck.com/dazz/wie-man-dinge-gut-benennt\"\u003eSlides\u003c/a\u003e | \u003ca href=\"https://www.youtube.com/watch?v=MP68H0c6VgI\"\u003e\u003c/a\u003e | \u003ca href=\"https://www.youtube.com/watch?v=VB-38COTgrg\"\u003e\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2015\"\u003e2015\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003ePHPUK15 – \u003ca href=\"https://speakerdeck.com/dazz/naming-things\"\u003eNaming things – The art of writing meaningful code\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch2 id=\"2014\"\u003e2014\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://www.bephpug.de/\"\u003eBePhpUg\u003c/a\u003e – \u003ca href=\"https://speakerdeck.com/dazz/nomen-est-omen-naming-things-considered-hard\"\u003eNomen est Omen - Naming things considered hard\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e","tags":[],"text":"2025 symfony ug cologne – running symfony in a multi-process container phpday verona 2025 – running php-applications in a multi-process container symfonylive berlin 2025 – running symfony in a multi-process container 2024 symfonycon vienna 2024 – running symfony in a multi-process container | slides #phpdd24 – running php-applications in a multi-process container |  symfony user group berlin - s6-overlay: symfony in a multi-process container 2023 symfonycon brussels 2023 – from chaos to control: exception handling in symfony symfonylive berlin 2023 – von chaos zu kontrolle: exception handling in symfony symfony user group berlin - from chaos to control: exception handling in symfony 2018 symfonylive berlin 2018 – wie man dinge gut benennt | slides |  |  2015 phpuk15 – naming things – the art of writing meaningful code 2014 bephpug – nomen est omen - naming things considered hard ","title":"talks"},{"date":"0001-01-01","image":"","imageAlt":"","link":"https://blog.dazzlog.de/ui/","summary":"\u003ch2 id=\"admonition\"\u003eAdmonition\u003c/h2\u003e\n\u003cdiv class=\"details admonition note open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#Xf040;\u003c/i\u003e This is a note\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eUse the note shortcode when you want to draw attention to information subtly. note is intended to be less of an interruption in content than is warning.\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition abstract open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf15c;\u003c/i\u003e This is a abstract\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition info open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf05a;\u003c/i\u003e This is an info\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition tip open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf400;\u003c/i\u003e This is a tip\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eUse the tip shortcode when you want to give the reader advice. tip, like note, is intended to be less of an interruption in content than is warning.\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition success open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf058;\u003c/i\u003e This is a success\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition question open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf128;\u003c/i\u003e This is a question\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition warning open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf071;\u003c/i\u003e This is an warning\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eUse the warning shortcode when you want to draw the user’s attention to something important. A good usage example is for articulating breaking changes in Hugo versions, known bugs, or templating “gotchas.”\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition failure open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf057;\u003c/i\u003e This is a failure\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition danger open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xfb8a;\u003c/i\u003e This is danger\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition bug open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf188;\u003c/i\u003e This is a bug\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition example open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#Xf03a;\u003c/i\u003e This is an example\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition quote open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf10d;\u003c/i\u003e This is a quote\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition symfony open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xe756;\u003c/i\u003e This is a symfony\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition php open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xe73d;\u003c/i\u003e This is a php\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition docker open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf308;\u003c/i\u003e This is a docker\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003cdiv class=\"details admonition tldr open\"\u003e\n  \u003cdiv class=\"details-summary admonition-title\"\u003e\n    \u003ci class=\"icon \"\u003e\u0026#xf259;\u003c/i\u003e This is a tldr;\u003ci class=\"details-icon fas fa-angle-right fa-fw\"\u003e\u003c/i\u003e\n  \u003c/div\u003e\n  \u003cdiv class=\"details-content\"\u003e\n    \u003cdiv class=\"admonition-content\"\u003eLorem Ipsum\u003c/div\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\n\u003ch2 id=\"fontawesome-cheat-sheet\"\u003eFontAwesome cheat sheet\u003c/h2\u003e\n\u003cp\u003e\u003ca href=\"https://forkaweso.me/Fork-Awesome/cheatsheet/\"\u003ehttps://forkaweso.me/Fork-Awesome/cheatsheet/\u003c/a\u003e\u003c/p\u003e","tags":["ui"],"text":"admonition \u0026#xf040; this is a note use the note shortcode when you want to draw attention to information subtly. note is intended to be less of an interruption in content than is warning. \u0026#xf15c; this is a abstract lorem ipsum \u0026#xf05a; this is an info lorem ipsum \u0026#xf400; this is a tip use the tip shortcode when you want to give the reader advice. tip, like note, is intended to be less of an interruption in content than is warning. \u0026#xf058; this is a success lorem ipsum \u0026#xf128; this is a question lorem ipsum \u0026#xf071; this is an warning use the warning shortcode when you want to draw the user’s attention to something important. a good usage example is for articulating breaking changes in hugo versions, known bugs, or templating “gotchas.” \u0026#xf057; this is a failure lorem ipsum \u0026#xfb8a; this is danger lorem ipsum \u0026#xf188; this is a bug lorem ipsum \u0026#xf03a; this is an example lorem ipsum \u0026#xf10d; this is a quote lorem ipsum \u0026#xe756; this is a symfony lorem ipsum \u0026#xe73d; this is a php lorem ipsum \u0026#xf308; this is a docker lorem ipsum \u0026#xf259; this is a tldr; lorem ipsum fontawesome cheat sheet https://forkaweso.me/fork-awesome/cheatsheet/\n","title":"ui"}]}